---
layout: post
title: "Ä°nsan Geri Bildirimi ile PekiÅŸtirmeli Ã–ÄŸrenme"
author: "MMA"
comments: true
---

* [Genel BakÄ±ÅŸ](#overview)
* [PekiÅŸtirmeli Ã–ÄŸrenme'nin Temelleri](#basics-of-rl)
* [EÄŸitim](#training)
  * [Ã–nceden EÄŸitilmiÅŸ Modeller kullanÄ±larak Dil Modeli OluÅŸturma](#pretraining-language-models)
  * [Ã–dÃ¼l Modeli](#reward-model)
  * [Dil Modeline, PekiÅŸtirmeli Ã–ÄŸrenme ile Ä°nce-Ayar Ã‡ekme](#fine-tuning-the-lm-with-rl)
* [YanlÄ±lÄ±k](#bias)
* [Ä°nce Ayar Ã‡ekmek iÃ§in PekiÅŸtirmeli Ã–ÄŸrenme'ye karÅŸÄ± Denetimli Ã–ÄŸrenme](#reinforcement-learning-vs-supervised-learning-for-finetuning)
* [KullanÄ±m SenaryolarÄ±](#use-cases)
* [Referanslar](#references)
  
## Genel BakÄ±ÅŸ {#overview}

* [Ä°nsan Geri Bildiriminden PekiÅŸtirmeli Ã–ÄŸrenme (Reinforcement Learning from Human Feedback)](https://openai.com/research/learning-from-human-preferences){:target="_blank"} ve ["Ä°nsan tercihlerinden derin pekiÅŸtirmeli Ã¶ÄŸrenme (Deep reinforcement learning from human preferences)"](https://arxiv.org/abs/1706.03741){:target="_blank"}, kavramÄ± tanÄ±tan ilk kaynaklardÄ±r.
* Ä°nsan Geri Bildirimi ile PekiÅŸtirmeli Ã–ÄŸrenme'nin (Reinforcement Learning with Human Feedback - RLHF) arkasÄ±ndaki temel fikir, Ã¶nceden eÄŸitilmiÅŸ bir dil modeli almak ve Ã§Ä±ktÄ±larÄ± insanlarÄ±n sÄ±ralamasÄ±nÄ± saÄŸlamaktÄ±r.
* RLHF, her iki pekiÅŸtirmeli Ã¶ÄŸrenme algoritmasÄ±nÄ± insan girdisiyle (human input) birleÅŸtirerek modelin Ã¶ÄŸrenmesine ve performansÄ±nÄ± artÄ±rmasÄ±na yardÄ±mcÄ± olabilecek insan geri bildirimi ile dil modellerini optimize edebilir.
* RLHF, insan geri bildirimlerini dahil ederek, dil modellerinin doÄŸal dili daha iyi anlamasÄ±na ve Ã¼retmesine yardÄ±mcÄ± olmanÄ±n yanÄ± sÄ±ra metin sÄ±nÄ±flandÄ±rmasÄ± (text classification) veya dil Ã§evirisi (language translation) gibi belirli gÃ¶revleri gerÃ§ekleÅŸtirme becerilerini geliÅŸtirmesine yardÄ±mcÄ± olabilir.
* Ek olarak, RLHF, insanlarÄ±n modeli daha adil ve kapsayÄ±cÄ± (inclusive) bir dil kullanÄ±mÄ±na doÄŸru dÃ¼zeltmesine ve yÃ¶nlendirmesine izin vererek dil modellerindeki yanlÄ±lÄ±k (bias) sorununu hafifletmeye de yardÄ±mcÄ± olabilir.
* AÅŸaÄŸÄ±da RLHF'nin inceliklerini inceleyelim!

## PekiÅŸtirmeli Ã–ÄŸrenme'nin Temelleri {#basics-of-rl}

* RLHF'de pekiÅŸtirmeli Ã¶ÄŸrenmenin neden kullanÄ±ldÄ±ÄŸÄ±nÄ± anlamak iÃ§in, bu Ã¶ÄŸrenme tÃ¼rÃ¼nÃ¼n masaya ne getirdiÄŸini daha iyi anlamamÄ±z gerekmektedir.
* PekiÅŸtirmeli Ã¶ÄŸrenme, aÅŸaÄŸÄ±da gÃ¶sterildiÄŸi gibi ([kaynak](https://www.youtube.com/watch?v=2MBJOuVq380){:target="_blank"}) bir ajanÄ±n (agent) Ã§evre ile etkileÅŸime girdiÄŸi temellere sahiptir:

![](https://vinija.ai/toolkit/assets/rlhf/1.png)

* Ajan, tek bir eylemde (action) bulunarak Ã§evre (environment) ile etkileÅŸime girer ve Ã§evre bir durum (status) ve bir Ã¶dÃ¼l (reward) dÃ¶ndÃ¼rÃ¼r.
  * Burada Ã¶dÃ¼l, optimize etmek istediÄŸimiz hedeftir (target).
  * Ve durum, Ã§evrenin/dÃ¼nyanÄ±n mevcut zaman indeksindeki temsilidir.
  * Bu durumdan bir eyleme eÅŸleme (matching) gerÃ§ekleÅŸtirmek iÃ§in bir politika (policy) kullanÄ±lÄ±r.
* Åimdi BÃ¼yÃ¼k Dil Modelleri (Large Language Models - LLM) ile DoÄŸal Dil Ä°ÅŸleme (Natural Language Processing - NLP) gÃ¶revleri iÃ§in PekiÅŸtirmeli Ã–ÄŸrenmenin nasÄ±l kullanÄ±labileceÄŸi hakkÄ±nda konuÅŸalÄ±m.
* Bir Ã¶rnek verelim, bir model iÃ§in mizahÄ±, etikleri ya da gÃ¼venliÄŸi nasÄ±l kodlarsÄ±nÄ±z (encoding)?
* Bunlar, insanlarÄ±n kendi baÅŸlarÄ±na anladÄ±klarÄ± bazÄ± incelikleri iÃ§erir, ancak Ã¶zel kayÄ±p fonksiyonlarÄ± oluÅŸturarak bir model Ã¼zerinde eÄŸitebileceÄŸimiz bir ÅŸey deÄŸildir.
* Ä°nsan Geri Bildirimi ile PekiÅŸtirmeli Ã–ÄŸrenme'nin (Reinforcement Learning with Human Feedback - RLHF) devreye girdiÄŸi yer burasÄ±dÄ±r.

![](https://vinija.ai/toolkit/assets/rlhf/2.png)

YukarÄ±daki gÃ¶rÃ¼ntÃ¼ ([kaynak](https://www.youtube.com/watch?v=2MBJOuVq380){:target="_blank"}), RLHF modelinin hem bir Dil Modelinden (Language model) hem de insan etiketinden (human annotation) girdileri nasÄ±l aldÄ±ÄŸÄ±nÄ± ve her ikisinden de ayrÄ± ayrÄ± daha iyi bir yanÄ±t oluÅŸturduÄŸunu gÃ¶stermektedir.

## EÄŸitim {#training}

* Ã–nce RLHF'ye Ã¼st dÃ¼zeyde bakarak iÅŸe baÅŸlayalÄ±m ve Ã¶nce tÃ¼m baÄŸlamÄ± ve gerÃ§ekleri gÃ¶z Ã¶nÃ¼ne alalÄ±m.
* RLHF, birden Ã§ok modelin eÄŸitimini ve farklÄ± daÄŸÄ±tÄ±m (deployment) aÅŸamalarÄ±nÄ± gerektirdiÄŸinden oldukÃ§a karmaÅŸÄ±k olabilir.
* GPT-4'e, ChatGPT'e ve InstructGPT'e, (OpenAI tarafÄ±ndan) RLHF ile ince-ayar Ã§ekildiÄŸinden (fine-tuning), eÄŸitim adÄ±mlarÄ±na bakarak daha derine inelim.
* RLHF, modelleri daha gÃ¼venli ve daha doÄŸru hale getirmek ve modelden oluÅŸturulan Ã§Ä±ktÄ± metninin gÃ¼venli ve kullanÄ±cÄ±larÄ±na daha uygun olmasÄ±nÄ± saÄŸlamak iÃ§in tasarlanmÄ±ÅŸtÄ±r.
* YZ ajanÄ± (AI agent), Ã§evrede (environment) rastgele kararlar alarak baÅŸlar.
* Periyodik olarak, bir insan sÄ±ralayandÄ±rÄ±cÄ±sÄ± (human ranker) iki video klip alacak ve mevcut gÃ¶reve hangi klibin daha uygun olduÄŸuna karar verecek.
* YZ ajanÄ± aynÄ± anda elindeki gÃ¶revine ait hedefin bir modelini oluÅŸturacak ve PekiÅŸtirmeli Ã–ÄŸrenme'yi kullanarak bu modeli iyileÅŸtirecektir.
* YZ ajanÄ± davranÄ±ÅŸÄ± Ã¶ÄŸrendiÄŸinde, yalnÄ±zca emin olmadÄ±ÄŸÄ± videolar hakkÄ±nda insanlardan geri bildirim istemeye baÅŸlayacak ve bÃ¶ylelikle anlayÄ±ÅŸÄ±nÄ± daha da geliÅŸtirecektir.
* Bu dÃ¶ngÃ¼sel davranÄ±ÅŸ, [OpenAI'Ä±n websayfasÄ±ndan](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/){:target="_blank"} alÄ±nan aÅŸaÄŸÄ±daki gÃ¶rselde gÃ¶rsel olarak gÃ¶rÃ¼lebilir:

![](https://vinija.ai/toolkit/assets/rlhf/3.png)

* OpenAI ÅŸirketi, mÃ¼ÅŸterilerinin API'lerini kullanarak modele gÃ¶nderdikleri istemleri (prompts) kullandÄ± ve model iÃ§in istenen birkaÃ§ Ã§Ä±ktÄ±yÄ± manuel olarak sÄ±ralayarak insan geri bildirimi aldÄ±.
* OpenAI ÅŸirketi, dil modeline ince-ayar Ã§ekmek iÃ§in mÃ¼ÅŸterileri tarafÄ±ndan GPT-3 API aracÄ±lÄ±ÄŸÄ±yla gÃ¶nderilen istemleri kullandÄ±.
* Ä°stemler, modelin yÃ¼ksek kaliteli Ã§Ä±ktÄ±lar Ã¼retmesini saÄŸlamak iÃ§in insan deÄŸerlendiricileri tarafÄ±ndan dÃ¼zenlendi ve manuel olarak sÄ±ralandÄ±.
* Bu sÃ¼reÃ§, denetimli Ã¶ÄŸrenme (supervised learning) olarak bilinir ve burada model, doÄŸruluÄŸunu ve performansÄ±nÄ± artÄ±rmak iÃ§in etiketli veriler kullanÄ±larak eÄŸitilir.
* OpenAI ÅŸirketi, modele mÃ¼ÅŸteri istemleriyle ince-ayar Ã§ekerek, belirli bir isteme yanÄ±t olarak ilgili ve tutarlÄ± metin oluÅŸturmasÄ± iÃ§in GPT-3'Ã¼ daha etkili hale getirmeyi amaÃ§ladÄ±.
* GÃ¶rev, YZ ajanÄ±na nasÄ±l ters takla atÄ±lacaÄŸÄ±nÄ± Ã¶ÄŸretmek olduÄŸunda, OpenAI, YZ ajanÄ±nÄ±n 900 bitlik geri bildirime ihtiyaÃ§ duyduÄŸunu buldu ve bu da bir insan zamanÄ±nÄ±n bir saatinden daha azÄ±na denk geliyordu.
* Bu algoritmanÄ±n karÅŸÄ±laÅŸtÄ±ÄŸÄ± zorluk, yalnÄ±zca insan geri bildirimi kadar iyi olmasÄ±dÄ±r.
* Neden her zaman RLHF' kullanmadÄ±ÄŸÄ±mÄ±zÄ± merak ediyor olabilirsiniz. PekiÅŸtirmeli Ã–ÄŸrenmeyi eÄŸitmek iÃ§in her zaman insanlara gÃ¼venmek zamanla bir darboÄŸaza (bottleneck) dÃ¶nÃ¼ÅŸtÃ¼ÄŸÃ¼ iÃ§in, PekiÅŸtirmeli Ã¶ÄŸrenme modelleri zayÄ±f Ã¶lÃ§eklenmektedir (scaling). _(Mustafa Murat ARAT: Genel olarak makine Ã¶ÄŸrenmesinin Ã¶nemli avantajlarÄ±ndan biri, hesaplama kaynaklarÄ±nÄ±n (computational resources) mevcudiyeti ile Ã¶lÃ§eklenebilme yeteneÄŸidir. Bilgisayarlar daha hÄ±zlÄ± bÃ¼yÃ¼dÃ¼kÃ§e ve daha fazla veri elde edilebilir oldukÃ§a daha bÃ¼yÃ¼k makine Ã¶ÄŸrenmesi modellerini daha hÄ±zlÄ± eÄŸitebilirsiniz. PekiÅŸtirmeli Ã–ÄŸrenme sistemlerini eÄŸitmek iÃ§in insanlara gÃ¼venmek bir darboÄŸaza dÃ¶nÃ¼ÅŸÃ¼r. Bu nedenle Ã§oÄŸu RLHF sistemi, otomatik ve insan tarafÄ±ndan saÄŸlanan Ã¶dÃ¼l sinyallerinin bir kombinasyonuna gÃ¼venir. OtomatikleÅŸtirilmiÅŸ Ã¶dÃ¼l sistemi, RL ajanÄ±na temel geri bildirimi saÄŸlar. Ä°nsan olan bir sÃ¼pervizÃ¶r, ara sÄ±ra ekstra bir Ã¶dÃ¼l/ceza sinyali saÄŸlayarak veya bir Ã¶dÃ¼l modelini (reward model) eÄŸitmek Ã¼zere gereken verileri saÄŸlayarak yardÄ±mcÄ± olur. Pizza piÅŸiren bir robot yapmak istediÄŸinizi varsayalÄ±m. Ã–lÃ§Ã¼lebilir Ã¶ÄŸelerden bazÄ±larÄ±nÄ± otomatikleÅŸtirilmiÅŸ Ã¶dÃ¼l sistemine entegre edebilirsiniz. (Ã¶r. kabuk kalÄ±nlÄ±ÄŸÄ±, sos ve peynir miktarÄ± vb.). Ancak pizzanÄ±n lezzetli olduÄŸundan emin olmak iÃ§in bir insanÄ±n damak tadÄ±na ihtiyaÃ§ duyarsÄ±nÄ±z ve eÄŸitim sÄ±rasÄ±nda robotun yaptÄ±ÄŸÄ± pizzalar bu insan tarafÄ±ndan skorlanÄ±r.)_

![](https://vinija.ai/toolkit/assets/rlhf/5.png)

* "Verilerin manuel olarak etiketlenmesi yavaÅŸ ve pahalÄ±dÄ±r, bu nedenle denetimsiz Ã¶ÄŸrenme, makine Ã¶ÄŸrenimi araÅŸtÄ±rmacÄ±larÄ±nÄ±n her zaman uzun sÃ¼redir aradÄ±ÄŸÄ± bir hedef olmuÅŸtur." [bdtechtalks](https://bdtechtalks.com/2023/01/16/what-is-rlhf/){:target="_blank"}
* EÄŸitim sÃ¼recini, buradaki [kaynaktan](https://huggingface.co/blog/rlhf){:target="_blank"} referans aldÄ±ÄŸÄ±mÄ±z Ã¼Ã§ adÄ±ma ayÄ±racaÄŸÄ±z.

### Ã–nceden EÄŸitilmiÅŸ Modeller kullanÄ±larak Dil Modeli OluÅŸturma {#pretraining-language-models}

* BildiÄŸimiz gibi, dil modelleri farklÄ± parametrelere sahip Ã§eÅŸitli modeller kullanÄ±larak Ã¶nceden eÄŸitilmiÅŸtir (pre-trained) ve belirli gÃ¶revler iÃ§in ince-ayar Ã§ekilebilir (fine-tuned).
* Bunun RLHF ile nasÄ±l iliÅŸkili olduÄŸuna daha fazla bakalÄ±m.
* Bir Ã¶dÃ¼l modeli (reward model) eÄŸitmek iÃ§in veri Ã¼retmek, insan tercihlerini sisteme entegre etmek iÃ§in gereklidir.
* Ancak, RLHF eÄŸitimindeki seÃ§eneklerin tasarÄ±m uzayÄ± tam olarak araÅŸtÄ±rÄ±lmadÄ±ÄŸÄ±ndan, RLHF'ye baÅŸlamak iÃ§in hangi modelin en iyi olduÄŸu konusunda net bir cevap yoktur.

![](https://vinija.ai/toolkit/assets/rlhf/4.png)

* YukarÄ±daki gÃ¶rÃ¼ntÃ¼ ([kaynak](https://huggingface.co/blog/rlhf){:target="_blank"}), bir dil modelinin Ã¶nceden-eÄŸitiminin (pre-training) iÃ§ iÅŸleyiÅŸini gÃ¶stermektedir.
* EndÃ¼stri deneyleri, 10 Milyar ila 280 Milyar parametre arasÄ±nda deÄŸiÅŸmiÅŸtir, ancak endÃ¼stride kullanÄ±lacak en iyi model boyutu konusunda henÃ¼z bir kesin cevap yoktur.
* Ek olarak, ÅŸirketler insanlara mevcut istemlere yanÄ±t yazmalarÄ± iÃ§in para Ã¶demesi yapabilir ve bu veriler daha sonra eÄŸitim iÃ§in kullanÄ±labilir.
  * Buradaki dezavantaj, pahalÄ± olabilmesidir.
  
### Ã–dÃ¼l Modeli {#reward-model}

* RLHF'nin en Ã¶nemli gÃ¶revi, insan tercihlerine dayalÄ± olarak girdi metnine skaler bir Ã¶dÃ¼l (reward) atayan bir Ã¶dÃ¼l modeli (reward model - RM) oluÅŸturmaktÄ±r _(Mustafa Murat ARAT: Denetimsiz Ã¶ÄŸrenme yoluyla Ã¶nceden eÄŸitilmiÅŸ bir BÃ¼yÃ¼k Dil Modeli (Large Language Model), halihazÄ±rda dilin saÄŸlam bir modeline sahip olacak ve tutarlÄ± Ã§Ä±ktÄ±lar yaratacaktÄ±r, ancak bu Ã§Ä±ktÄ±larÄ±n bir kÄ±smÄ± veya Ã§oÄŸu kullanÄ±cÄ±larÄ±n amaÃ§ ve niyetleriyle uyumlu olmayabilir. Bu nedenle, ikinci aÅŸamada PekiÅŸtirmeli Ã–ÄŸrenme sistemi iÃ§in ikinci bir model olan bir Ã¶dÃ¼l modeli oluÅŸturulur. Bu Ã¶dÃ¼l modeli bir baÅŸka Makine Ã¶ÄŸrenmesi modelidir.)_
* Bu Ã¶dÃ¼l modeli, uÃ§tan uca bir Dil Modeli veya modÃ¼ler bir sistem olabilir, ve bu Ã¶dÃ¼l modelinin eÄŸitimi iÃ§in kullanÄ±labilecek veri kÃ¼mesi, Dil Modelinden elde edilen metin Ã§Ä±ktÄ±sÄ± ve bu Ã§Ä±ktÄ±nÄ±n insan tarafÄ±ndan sÄ±ralanmasÄ± esnasÄ±nda verilmiÅŸ skorlarÄ±n Ã§iftlerinden oluÅŸur. 

![](https://vinija.ai/toolkit/assets/rlhf/6.png)

* YukarÄ±daki gÃ¶rÃ¼ntÃ¼ ([kaynak](https://huggingface.co/blog/rlhf){:target="_blank"}), Ã¶dÃ¼l modelinin dahili olarak nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± gÃ¶sterir.
* YukarÄ±daki gÃ¶rÃ¼ntÃ¼deki Ã¶dÃ¼l modeline baktÄ±ÄŸÄ±mÄ±zda, amacÄ±n, bazÄ± metin sekanslarÄ±nÄ±n (text sequence) girdisinden skaler bir Ã¶dÃ¼l deÄŸerine eÅŸlenen bir model elde etmek istediÄŸimizi gÃ¶rebiliriz.
* PekiÅŸtirmeli Ã¶ÄŸrenme'nin tek bir skaler deÄŸer aldÄ±ÄŸÄ± ve bu skaleri, Ã§evresi (environment) aracÄ±lÄ±ÄŸÄ±yla zaman iÃ§inde optimize ettiÄŸi bilinmektedir.
* Ã–dÃ¼l modelinin eÄŸitimi de bir veri kÃ¼mesiyle baÅŸlar, ancak bunun Ã¶nceden eÄŸitilmiÅŸ dil modeli iÃ§in kullanÄ±lan veri kÃ¼mesinden farklÄ± bir veri kÃ¼mesi olduÄŸuna dikkat ediniz.
* Buradaki veri kÃ¼mesi, daha Ã§ok belirli tercihlere odaklanÄ±r ve istemlerden (prompts) oluÅŸan bir girdi veri kÃ¼mesidir.
* Modelin kullanÄ±lacaÄŸÄ± belirli bir kullanÄ±m durumu iÃ§in, dil modelinin Ã¶nceden Ã¼zerinde eÄŸitildiÄŸi istemlerden Ã§ok daha kÃ¼Ã§Ã¼k istemler iÃ§erir.
* Dil modeli verilen istem iÃ§in bir metni Ã¼retecek ve ardÄ±ndan bu metin sÄ±ralandÄ±rÄ±lacaktÄ±r (ranked). 
* Ã‡oÄŸu zaman, Ã§eÅŸitli sÄ±ralamalar oluÅŸturmak iÃ§in birden fazla model kullanabilirsiniz ve bunlarÄ± sÄ±ralayan bir insan olabilir.
* Ä°nsan skorlamasÄ±ndan geri bildirim almak Ã¼zere bir Ã¶dÃ¼l modelinin eÄŸitiminin Ã¶rnek bir arayÃ¼zÃ¼nÃ¼, ChatGPT'yi kullandÄ±ÄŸÄ±nÄ±zda "Ã§ok iyi (thumbs up) ğŸ‘ğŸ½ " veya "Ã§ok kÃ¶tÃ¼ (thumbs down) ğŸ‘ğŸ½" simgelerinde gÃ¶rebilirsiniz.
* BÃ¶ylelikle model Ã§Ä±ktÄ±sÄ±nÄ±n sÄ±ralamasÄ±nÄ± kitle kaynaklÄ± (crowd-sources) olarak Ã¶ÄŸrenir. 

### Dil Modeline, PekiÅŸtirmeli Ã–ÄŸrenme ile Ä°nce-Ayar Ã‡ekme {#fine-tuning-the-lm-with-rl}

![](https://vinija.ai/toolkit/assets/rlhf/7.png)

* YukarÄ±daki gÃ¶rÃ¼ntÃ¼ ([kaynak](https://huggingface.co/blog/rlhf){:target="_blank"}), ince-ayar Ã§ekmenin Ã¶dÃ¼l modeliyle nasÄ±l Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ± aÃ§Ä±klamaktadÄ±r.
* Burada istemlerden oluÅŸan veri kÃ¼mesini alÄ±yoruz (kullanÄ±cÄ±nÄ±n sÃ¶ylediÄŸi bir ÅŸey veya modelin iyi Ã¼retebilmesini istediÄŸimiz bir ÅŸey).
* ArdÄ±ndan, bir istem iÃ§in ayarlanmÄ±ÅŸ (tuned) bir dil modeli olan PekiÅŸtirmeli Ã–ÄŸrenme PolitikasÄ±na gÃ¶re bir Ã§Ä±ktÄ± Ã¼retilir.
* Daha sonra, bu Ã§Ä±ktÄ±, bir skaler deÄŸer Ã¼reten Ã¶dÃ¼l modeline gÃ¶nderilir.
* Bu sÃ¼reÃ§ bir geri bildirim dÃ¶ngÃ¼sÃ¼nde gerÃ§ekleÅŸtirilir, bÃ¶ylece zaman iÃ§inde gÃ¼ncellemeler yaÅŸanÄ±r.
* Burada kullanÄ±lan cezalandÄ±rÄ±cÄ± fonksiyonu (penalty function), Kullback-Leibler IraksamasÄ±nÄ±n (Kullback-Leibler (KL) divergence)'nÄ±n bir varyasyonudur.
* Kullback-Leibler (KL) Ä±raksamasÄ±, iki olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ± arasÄ±ndaki farkÄ±n bir Ã¶lÃ§Ã¼sÃ¼dÃ¼r.
  * BÃ¶ylece, RLHF ile KL Ä±raksamanÄ±, bir ajanÄ±n mevcut politikasÄ±nÄ±n olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±nÄ±, istenilen davranÄ±ÅŸÄ± temsil eden bir referans daÄŸÄ±lÄ±mÄ±yla karÅŸÄ±laÅŸtÄ±rmak iÃ§in kullanÄ±labilir.
* Bu, dil modelinin anlamsÄ±z sÃ¶zler Ã¼retmesini ve yÃ¼ksek bir Ã¶dÃ¼l almasÄ±nÄ± engeller. Yani modeli sadece yÃ¼ksek Ã¶dÃ¼l kazanmaya odaklamakla kalmaz, bunun sonucunda doÄŸru bir metin Ã¼retmesini de saÄŸlar.
* Ek olarak, RLHF'e Proksimal Politika Optimizasyonu (Proximal Policy Optimization - PPO) ile ince-ayar Ã§ekilebilir.
  * Proksimal Politika Optimizasyonu (Proximal Policy Optimization - PPO), yÃ¼ksek boyutlu durum (status) ve eylem (action) uzaylarÄ±na sahip karmaÅŸÄ±k Ã§evrelerdeki (environment) politikalarÄ± (policies) verimli bir ÅŸekilde optimize etme yeteneÄŸi nedeniyle, Ä°nsan Geri Bildirimi ile PekiÅŸtirmeli Ã–ÄŸrenme'ye (RLHF) ince-ayar Ã§ekmek iÃ§in sÄ±klÄ±kla kullanÄ±lan popÃ¼ler bir pekiÅŸtirmeli Ã¶ÄŸrenme algoritmasÄ±dÄ±r.
* PPO, hem insan geri bildiriminden hem de deneme-yanÄ±lma keÅŸfinden (exploration) Ã¶ÄŸrenmesi gereken RLHF ajanlarÄ± iÃ§in Ã¶nemli olan keÅŸif (exploration) ile mevcut bilgiden istifadeyi (exploitation) eÄŸitim esnasÄ±nda verimli bir ÅŸekilde dengeler.
* PPO'nun RLHF'de kullanÄ±lmasÄ±, ajanÄ± hem insan geri bildiriminden hem de pekiÅŸtirmeli Ã¶ÄŸrenmeden Ã¶ÄŸrenebildiÄŸinden, daha hÄ±zlÄ± ve daha saÄŸlam (robust) Ã¶ÄŸrenmeyle sonuÃ§lanabilir.

## YanlÄ±lÄ±k {#bias}

* Åimdi asÄ±l sorulacak soru, RLHF'nin modele yanlÄ±lÄ±k ekleyip ekleyemeyeceÄŸidir.
* Bu Ã¶nemlidir, Ã§Ã¼nkÃ¼ bÃ¼yÃ¼k konuÅŸmalÄ± (conversational) dil modellerinin RLHF tarafÄ±ndan ince ayar Ã§ekildiÄŸini ve arama motorundan (Bing) kelime dokÃ¼manlarÄ±na (Google docs, Notion vb.) kadar Ã§eÅŸitli uygulamalara dahil edildiÄŸini gÃ¶rÃ¼yoruz.
* YanÄ±t, evet, tÄ±pkÄ± insan girdisine sahip herhangi bir makine Ã¶ÄŸrenmesi yaklaÅŸÄ±mÄ±nda olduÄŸu gibi, RLHF'nin yanlÄ±lÄ±k oluÅŸturma potansiyeli vardÄ±r.
* GetirebileceÄŸi farklÄ± yanlÄ±lÄ±k biÃ§imlerine gÃ¶z atalÄ±m:
* SeÃ§im yanlÄ±lÄ±ÄŸÄ± (Selection Bias):
  * RLHF, kendi Ã¶nyargÄ±larÄ± ve tercihleri olan insan deÄŸerlendiricilerden gelen geri bildirimlere gÃ¼venmektedir, bu nedenle ajan, gerÃ§ek dÃ¼nyada karÅŸÄ±laÅŸacaÄŸÄ± doÄŸru davranÄ±ÅŸlara ve sonuÃ§lara maruz kalmayabilir.
* Onay yanlÄ±lÄ±ÄŸÄ± (Confirmation bias):
  * Ä°nsan deÄŸerlendiricilerin, ajanÄ±n performansÄ±na dayalÄ± objektif geri bildirim saÄŸlamak yerine, mevcut inanÃ§larÄ±nÄ± veya beklentilerini doÄŸrulayan geri bildirim saÄŸlama olasÄ±lÄ±ÄŸÄ± daha yÃ¼ksek olabilir.
  * Bu, ajanÄ±n uzun vadede optimal olmayan veya arzu edilmeyen belirli davranÄ±ÅŸlar veya sonuÃ§lar iÃ§in gÃ¼Ã§lendirilmesine yol aÃ§abilir.
* DeÄŸerlendiriciler arasÄ± deÄŸiÅŸkenlik (Inter-rater variability):
  * FarklÄ± insan deÄŸerlendiriciler, ajanÄ±n performansÄ±nÄ±n kalitesi hakkÄ±nda farklÄ± gÃ¶rÃ¼ÅŸlere veya yargÄ±lara sahip olabilir ve bu da ajanÄ±n aldÄ±ÄŸÄ± geri bildirimde tutarsÄ±zlÄ±ÄŸa yol aÃ§ar.
  * Bu, ajanÄ± etkili bir ÅŸekilde eÄŸitmeyi zorlaÅŸtÄ±rabilir ve optimal olmayan performansa yol aÃ§abilir.
* SÄ±nÄ±rlÄ± geri bildirim (Limited feedback):
  * Ä°nsan deÄŸerlendiriciler, ajanÄ±n performansÄ±nÄ±n tÃ¼m yÃ¶nleri hakkÄ±nda geri bildirim saÄŸlayamayabilir, bu da ajanÄ±n Ã¶ÄŸrenmesinde boÅŸluklara ve belirli durumlarda potansiyel olarak yetersiz performans gÃ¶stermesine yol aÃ§ar.
* ArtÄ±k RLHF ile mÃ¼mkÃ¼n olan farklÄ± yanlÄ±lÄ±k tÃ¼rlerini gÃ¶rdÃ¼ÄŸÃ¼mÃ¼ze gÃ¶re, bunlarÄ± azaltmanÄ±n yollarÄ±na bakalÄ±m:
* Ã‡eÅŸitli deÄŸerlendirici seÃ§imi:
  * TÄ±pkÄ± iÅŸyerinde olduÄŸu gibi, farklÄ± geÃ§miÅŸlere ve bakÄ±ÅŸ aÃ§Ä±larÄ±na sahip deÄŸerlendiricilerin seÃ§ilmesi, geri bildirimdeki yanlÄ±lÄ±ÄŸÄ±n azaltÄ±lmasÄ±na yardÄ±mcÄ± olabilir.
  * Bu, farklÄ± demografik gruplardan, bÃ¶lgelerden veya sektÃ¶rlerden deÄŸerlendiriciler iÅŸe alÄ±narak baÅŸarÄ±labilir.
* Fikir birliÄŸi deÄŸerlendirmesi:
  * Birden Ã§ok deÄŸerlendiricinin aynÄ± gÃ¶rev hakkÄ±nda geri bildirim saÄŸladÄ±ÄŸÄ± fikir-birliÄŸi (consensus) deÄŸerlendirmesini kullanmak, bireysel yanlÄ±lÄ±klarÄ±n etkisini azaltmaya ve geri bildirimin gÃ¼venilirliÄŸini artÄ±rmaya yardÄ±mcÄ± olabilir.
  * Bu neredeyse deÄŸerlendirmeyi 'normalleÅŸtirmek' gibidir.
* DeÄŸerlendiricilerin kalibrasyonu:
  * DeÄŸerlendiricilere geri bildirimin nasÄ±l saÄŸlanacaÄŸÄ± konusunda eÄŸitim ve rehberlik saÄŸlayarak onlarÄ± kalibre etmek, geri bildirimin kalitesini ve tutarlÄ±lÄ±ÄŸÄ±nÄ± artÄ±rmaya yardÄ±mcÄ± olabilir.
* Geri bildirim sÃ¼recinin deÄŸerlendirilmesi:
  * Geri bildirimin kalitesi ve eÄŸitim sÃ¼recinin etkinliÄŸi de dahil olmak Ã¼zere geri bildirim sÃ¼recinin dÃ¼zenli olarak deÄŸerlendirilmesi, mevcut olabilecek yanlÄ±lÄ±klarÄ±n belirlenmesine ve ele alÄ±nmasÄ±na yardÄ±mcÄ± olabilir.
* AjanÄ±n performansÄ±nÄ±n deÄŸerlendirilmesi:
  * AjanÄ±n performansÄ±nÄ± Ã§eÅŸitli gÃ¶revlerde ve farklÄ± ortamlarda dÃ¼zenli olarak deÄŸerlendirmek, bu ajanÄ±n belirli Ã¶rneklere aÅŸÄ±rÄ± uyum (overfitting) gÃ¶stermemesini ve yeni durumlara genelleme yapabilmesini saÄŸlamaya yardÄ±mcÄ± olabilir.
* Geri bildirimi dengelemek:
  * Ä°nsan deÄŸerlendiricilerden gelen geri bildirimleri kendi kendine oynama (self-play) veya uzman gÃ¶sterileri (expert demonstrations) gibi diÄŸer geri bildirim kaynaklarÄ±yla dengelemek, geri bildirimdeki yanlÄ±lÄ±ÄŸÄ±n etkisini azaltmaya ve eÄŸitim verilerinin genel kalitesini iyileÅŸtirmeye yardÄ±mcÄ± olabilir.
  
## Ä°nce Ayar Ã‡ekmek iÃ§in PekiÅŸtirmeli Ã–ÄŸrenme'ye karÅŸÄ± Denetimli Ã–ÄŸrenme {#reinforcement-learning-vs-supervised-learning-for-finetuning}

Not: Bu bÃ¶lÃ¼m Sebastian Raschka'nÄ±n [bu baÄŸlantÄ±daki](https://www.linkedin.com/posts/sebastianraschka_ai-deeplearning-machinelearning-activity-7036333477658599424-rkSL?utm_source=share&utm_medium=member_desktop){:target="_blank"} gÃ¶nderisinden esinlenilmiÅŸtir ve aÅŸaÄŸÄ±daki gÃ¶rsel de [Sebastian Raschka](https://www.linkedin.com/posts/sebastianraschka_ai-deeplearning-machinelearning-activity-7036333477658599424-rkSL?utm_source=share&utm_medium=member_desktop){:target="_blank"}'ya aittir.

![](https://vinija.ai/toolkit/assets/rlhf/8.png)

* PekiÅŸtirmeli Ã¶ÄŸrenme, insan geri bildirimi tarafÄ±ndan saÄŸlanan etiketlere (labels) ihtiyaÃ§ duyar, bu nedenle, neden bu etiketleri yalnÄ±zca Denetimli Ã–ÄŸrenme ile birlikte kullanmadÄ±ÄŸÄ±mÄ±z sorusu ortaya Ã§Ä±kÄ±yor.
* Ä°ÅŸte gÃ¶nderide  bahsedilen 4 neden:
  1. Denetimli Ã–ÄŸrenme, gerÃ§ek etiket ile model Ã§Ä±ktÄ±sÄ± arasÄ±ndaki boÅŸluÄŸu azaltmaya odaklanÄ±r. Bu da, modelin sadece sÄ±ralamalarÄ± (ranks) ezberleyeceÄŸi ve muhtemelen anlamsÄ±z Ã§Ä±ktÄ±lar Ã¼reteceÄŸi anlamÄ±na gelir, Ã§Ã¼nkÃ¼ modelin odak noktasÄ± sÄ±ralamalarÄ± maksimize etmektir.
    * Daha Ã¶nce konuÅŸtuÄŸumuz gibi, Ã¶dÃ¼l modelinin yaptÄ±ÄŸÄ± budur ve KL Ä±raksamasÄ±nÄ±n yardÄ±mcÄ± olabileceÄŸi yer burasÄ±dÄ±r.
  2. Bu durumda, biri sÄ±ralama (rank) ve diÄŸeri Ã§Ä±ktÄ± iÃ§in olmak Ã¼zere iki kaybÄ± ortaklaÅŸa eÄŸitsek ne olur? Bu senaryo, ChatGPT veya diÄŸer konuÅŸma modellerinin sahip olduÄŸu her gÃ¶rev iÃ§in deÄŸil, yalnÄ±zca Soru ve Cevap (Q and A) gÃ¶revleri iÃ§in Ã§alÄ±ÅŸÄ±r.
  3. GPT'nin kendisi, bir sonraki sÃ¶zcÃ¼k tahmini iÃ§in Ã§apraz entropi (cross-entropy) kaybÄ±nÄ± kullanÄ±r. Bununla birlikte, RLHF ile standart kayÄ±p fonksiyonlarÄ±nÄ± deÄŸil, modelin RLHF'nin kullanÄ±ldÄ±ÄŸÄ± gÃ¶reve daha iyi hizmet etmesine yardÄ±mcÄ± olan amaÃ§ fonksiyonlarÄ± kullanÄ±rÄ±z, Ã¶rn. GÃ¼ven ve GÃ¼venlik (Trust and Safety).
    * Ek olarak, bir kelimeyi olumsuzlamak metnin anlamÄ±nÄ± tamamen deÄŸiÅŸtirebileceÄŸinden, burada en iyi ÅŸekilde kullanÄ±lmaz.
  4. "Deneysel olarak bakÄ±ldÄ±ÄŸÄ±nda, RLHF, denetimli Ã¶ÄŸrenmeden daha iyi performans gÃ¶sterme eÄŸilimindedir. Bunun nedeni, denetimli Ã¶ÄŸrenmenin andaÃ§ (token) dÃ¼zeyinde bir kayÄ±p fonksiyonu kullanmasÄ± (yani, bir metin pasajÄ± Ã¼zerinden toplanabilecek veya ortalamasÄ± alÄ±nabilecek) ve RL'nin tÃ¼m metin pasajÄ±nÄ± bir bÃ¼tÃ¼n olarak hesaba katmasÄ±dÄ±r."
  5. Problem ya denetimli Ã¶ÄŸrenme ya da RLHF deÄŸildir. InstructGPT ve ChatGPT her ikisini de kullanÄ±r. Ã–nce denetimli Ã¶ÄŸrenme ile ince ayar Ã§ekilir, ardÄ±ndan RLHF ile bu modeller gÃ¼ncellenir.
  
## KullanÄ±m SenaryolarÄ± {#use-cases}

* Åimdi farklÄ± Ã§alÄ±ÅŸmalarÄ±n bu metodolojiyi kendi kÃ¼Ã§Ã¼k dÃ¼zeltmeleri nasÄ±l kullandÄ±klarÄ±na bakalÄ±m ([kaynak](https://www.youtube.com/watch?v=2MBJOuVq380){:target="_blank"})
* ChatGPT gibi en yeni BÃ¼yÃ¼k Dil Modelleri, denetimli Ã¶ÄŸrenme yerine ince ayar Ã§ekmek iÃ§in RLHF'yi kullanma eÄŸilimindedir.
* Anthropic:
  * RLHF iÃ§in kullandÄ±klarÄ± ilk politika, yardÄ±mseverliÄŸi (helpfulness), dÃ¼rÃ¼stlÃ¼ÄŸÃ¼ (honesty) ve zararsÄ±zlÄ±ÄŸÄ± (harmlessness) (HHH olarak bilinir) iyileÅŸtirmeye yardÄ±mcÄ± olan baÄŸlam danÄ±tmasÄ±na (context distillation) sahiptir.
  * Tercih modelin Ã¶nceden-eÄŸitimi (Preference model pretraining - PMP): ikili sÄ±ralamalardan oluÅŸan veri kÃ¼mesi Ã¼zerinde Dil Modeline ince ayar Ã§ekme
* OpenAI InstructGPT, ChatGPT:
  * RLHF kullanarak Ã¶ncÃ¼lÃ¼k etti
  * Hem InstructGPT hem de ChatGPT, Ã¶nce Denetimli Ã–ÄŸrenme yoluyla modele ince ayar Ã§eker, ardÄ±ndan modeli RLHF ile gÃ¼nceller
  * Ä°nsan tarafÄ±ndan oluÅŸturulan baÅŸlangÄ±Ã§ Dil Modeli eÄŸitim metni, ardÄ±ndan PekiÅŸtirmeli Ã–ÄŸrenme politikasÄ±nÄ± bununla eÅŸleÅŸecek ÅŸekilde eÄŸitir
  * KapsamlÄ± bir ÅŸekilde insan etiketlerini (human annotation) kullanÄ±r
  * PPO (Proksimal Politika Optimizasyonu / Proximal Policy Optimization) kullanÄ±r
* DeepMind
  * PPO kullanmaz, algoritma iÃ§in PPO yerine Avantaj AktÃ¶r Kritik (Advantage Actor Critic - A2C) kullanÄ±r
  * Modelin yapmamasÄ± gereken ÅŸeyler Ã¼zerinde eÄŸitilmesinin yanÄ± sÄ±ra, farklÄ± kurallar ve tercihler Ã¼zerinde eÄŸitilir.
  
## Referanslar {#references}

* [Ä°nsan Geri Bildiriminden PekiÅŸtirmeli Ã–ÄŸrenme: SÄ±fÄ±rdan chatGPT'ye](https://www.youtube.com/watch?v=2MBJOuVq380)
* [Ä°nsan Geri Bildiriminden PekiÅŸtirmeli Ã–ÄŸrenme (RLHF) Ä°llÃ¼strasyonu](https://huggingface.co/blog/rlhf)
* [Sebastian Raschka'nÄ±n LinkedIn gÃ¶nderisi](https://www.linkedin.com/in/sebastianraschka?miniProfileUrn=urn%3Ali%3Afs_miniProfile%3AACoAAAxqHaUBEa6zzXN--gv-wd8ih0vevPvr9eU&lipi=urn%3Ali%3Apage%3Ad_flagship3_search_srp_all%3Bic8rQnV%2BTHqwI0K2TXBzzg%3D%3D)
