---
layout: post
title: "Ã–nceden EÄŸitilmiÅŸ Visual Transformer (ViT) modeline Ä°nce-Ayar Ã‡ekmek"
author: "MMA"
comments: true
---

SÄ±fÄ±rdan eÄŸitim (training from scratch), bir modelin tamamen yeni bir gÃ¶rev baÅŸtan sona eÄŸitilmesini iÃ§erir. Bu genellikle bÃ¼yÃ¼k veri setleri ve yÃ¼ksek hesaplama gÃ¼cÃ¼ (computation power) gerektirir. AyrÄ±ca, eÄŸitim sÃ¼reci genellikle gÃ¼nler veya haftalar sÃ¼rebilir.Bu yÃ¶ntem genellikle Ã¶zel bir gÃ¶rev veya dil modeli oluÅŸturmak isteyen araÅŸtÄ±rmacÄ±lar ve bÃ¼yÃ¼k ÅŸirketler tarafÄ±ndan kullanÄ±lÄ±r.

Ancak, bu iÅŸi hobi olarak yapan biri veya bir Ã¶ÄŸrenci iÃ§in bir modeli sÄ±fÄ±rdan oluÅŸturmak o kadar kolay deÄŸildir. BÃ¼yÃ¼k veri ve yÃ¼ksek hesaplama gÃ¼cÃ¼nÃ¼n yanÄ±nda, aynÄ± zamanda oluÅŸturulacak modelin mimarisini (architecture) belirleme ve bu modele ait hiperparametreleri ayarlama (hyperparameter tuning) sÃ¼reci de zorludur. 

Bu sebeple, transfer Ã¶ÄŸrenme (transfer learning) adÄ± verilen bir konsept literatÃ¼rde yerini almÄ±ÅŸtÄ±r. 

Ã–ÄŸrenimleri bir problemden yeni ve farklÄ± bir probleme uyarlamak Transfer Ã–ÄŸrenme fikrini temsil eder. ÅÃ¶yle dÃ¼ÅŸÃ¼nÃ¼rsek insanÄ±n Ã¶ÄŸrenmesi bÃ¼yÃ¼k Ã¶lÃ§Ã¼de bu Ã¶ÄŸrenme yaklaÅŸÄ±mÄ±na dayanmaktadÄ±r. Transfer Ã¶ÄŸrenimi sayesinde Java Ã¶ÄŸrenmek size oldukÃ§a kolay gelebilir Ã§Ã¼nkÃ¼ Ã¶ÄŸrenme sÃ¼recine girildiÄŸinde zaten programlama kavramlarÄ±nÄ± ve Python sÃ¶zdizimini anlÄ±yorsunuzdur.

AynÄ± mantÄ±k derin Ã¶ÄŸrenme (deep learning) iÃ§in de geÃ§erlidir. Transfer Ã¶ÄŸrenme, genellikle Ã¶nceden-eÄŸitilmiÅŸ (pre-trained) bir modelin (Ã¶rneÄŸin, Hugging Face tarafÄ±ndan saÄŸlanan bir dil modeli) Ã¶zel bir gÃ¶rev veya veri kÃ¼mesine uyarlanmasÄ±dÄ±r. DiÄŸer bir deyiÅŸle, Ã¶nceden eÄŸitilmiÅŸ bir modelin aÄŸÄ±rlÄ±klarÄ± yeni veriler Ã¼zerinde eÄŸitilir. BÃ¶ylelikle, Ã¶nceden eÄŸitilmiÅŸ model yeni bir gÃ¶rev iÃ§in hazÄ±r hale gelir. 

Ã–nceden eÄŸitilmiÅŸ bir model kullanmanÄ±n Ã¶nemli faydalarÄ± vardÄ±r. Hesaplama maliyetlerini ve karbon ayak izinizi azaltÄ±r ve sÄ±fÄ±rdan eÄŸitim almanÄ±za gerek kalmadan son teknoloji Ã¼rÃ¼nÃ¼ modelleri kullanmanÄ±za olanak tanÄ±r

ğŸ¤— Hugging Face Transformers, Ã§ok Ã§eÅŸitli gÃ¶revler iÃ§in (Ã¶rneÄŸin, doÄŸal dil iÅŸleme ve bilgisayarlÄ± gÃ¶rÃ¼) Ã¶nceden eÄŸitilmiÅŸ binlerce modele eriÅŸim saÄŸlar (https://huggingface.co/models). Ã–nceden eÄŸitilmiÅŸ bir model kullandÄ±ÄŸÄ±nÄ±zda, onu gÃ¶revinize Ã¶zel bir veri kÃ¼mesi Ã¼zerinde eÄŸitirsiniz. Bu, inanÄ±lmaz derecede gÃ¼Ã§lÃ¼ bir eÄŸitim tekniÄŸi olan ince-ayar (fine-tuning) olarak bilinir.

Transformer-tabanlÄ± modeller genellikle gÃ¶revden baÄŸÄ±msÄ±z gÃ¶vde (task-independent body) ve gÃ¶reve Ã¶zel kafa (task-specific head) olarak ikiye ayrÄ±lÄ±r. Genellikle gÃ¶revden baÄŸÄ±msÄ±z kÄ±sÄ±m, Hugging Face tarafÄ±ndan saÄŸlanan aÄŸÄ±rlÄ±klara (weights) sahiptir. Bu kÄ±sÄ±mdaki aÄŸÄ±rlÄ±klar dondurulmuÅŸtur ve herhangi bir gÃ¼ncellemeye (updates) sahip olmazlar. GÃ¶reve Ã¶zel kafa'da, elinizdeki gÃ¶rev iÃ§in ihtiyacÄ±nÄ±z kadar nÃ¶ron oluÅŸturulur ve sadece bu katmanda eÄŸitim Ã¶zel veri kÃ¼meniz kullanÄ±larak gerÃ§ekleÅŸtirilir.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/fine_tuning_example.png?raw=true)

Ancak, ince ayar, sinir aÄŸÄ±nÄ±n tamamÄ±nda veya yalnÄ±zca katmanlarÄ±nÄ±n bir alt kÃ¼mesinde yapÄ±labilir; bu durumda, ince ayarÄ± yapÄ±lmayan katmanlar "dondurulur (frozen)" (geri yayÄ±lÄ±m (backpropagation) adÄ±mÄ± sÄ±rasÄ±nda gÃ¼ncellenmez).

Ä°ÅŸte, bu tutorial'da Ã¶zel bir veri kÃ¼mesi (a custom dataset) iÃ§in Ã¶nceden eÄŸitilmiÅŸ bir modele ince ayar yapacaksÄ±nÄ±z.

Ä°lk olarak bu tutorial'da kullanaca

# Google Colab'e GiriÅŸ

Burada gerÃ§ekleÅŸtireceÄŸiniz analizleri GPU'ya sahip bir makinede yapmanÄ±zda fayda var. Ã‡Ã¼nkÃ¼ kullanÄ±lacak ViT modeli ve veri kÃ¼mesi oldukÃ§a bÃ¼yÃ¼k. Bu nedenle modele ince-ayar Ã§ekmek oldukÃ§a zaman alabilir. 

Ä°lk olarak bu tutorial'da kullanacaÄŸÄ±nÄ±z tÃ¼m Python kÃ¼tÃ¼phanelerini aÅŸaÄŸÄ±daki ÅŸekilde iÃ§e aktaralÄ±m:

```python
from google.colab import drive
import os
import copy
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
import torch
from datasets import load_dataset, Image, load_from_disk
from transformers import (ViTFeatureExtractor,
                          ViTForImageClassification,
                          TrainingArguments,
                          TrainerCallback,
                          Trainer,
                          AutoModelForImageClassification,
                          AutoFeatureExtractor)
import evaluate
from huggingface_hub import notebook_login, create_repo
```

Tabii ki, bu kÃ¼tÃ¼phaneler kiÅŸisel bilgisayarÄ±nÄ±zda veya Colab ortamÄ±nÄ±zda yoksa, Ã¶ncelikle bunlarÄ± yÃ¼klemeniz (install) etmeniz gerekmektedir:

```python
!pip3 install datasets
!pip install transformers
!pip install transformers[torch]
!pip install accelerate -U
!pip install evaluate
!pip install scikit-learn
```

Gerekli kÃ¼tÃ¼phaneler yÃ¼klendikten ve bu kÃ¼tÃ¼phaneler python ortamÄ±na iÃ§eri aktarÄ±ldÄ±ktan sonra, yapmanÄ±z gereken iÅŸlem Depolama (Storage) alanÄ±nÄ± ayarlamaktÄ±r.

Google Colab'in bir faydasÄ±, Google Drive'Ä±nÄ±za baÄŸlanmanÄ±za olanak saÄŸlamasÄ±dr. BÃ¶ylelikle, elinizdeki veri Drive'da barÄ±nÄ±rken, kodlarÄ±nÄ±zÄ± GPU destekli bir Jupyter Not Defterinde Ã§alÄ±ÅŸtÄ±rabilirsiniz.

Ã–ncelikle Google Drive'Ä± Colab'a baÄŸlayalÄ±m. Google Drive'Ä±nÄ±zÄ±n tamamÄ±nÄ± Colab'a baÄŸlamak iÃ§in `google.colab` kÃ¼tÃ¼phanesindeki `drive` modÃ¼lÃ¼nÃ¼ kullanabilirsiniz:

```python
drive.mount('/content/gdrive')
```

Google HesabÄ±nÄ±za eriÅŸim izni verdikten sonra Drive'a baÄŸlanabilirsiniz.

Drive baÄŸlandÄ±ktan sonra `"Mounted at /content/gdrive"` mesajÄ±nÄ± alÄ±rsÄ±nÄ±z ve dosya gezgini bÃ¶lmesinden Drive'Ä±nÄ±zÄ±n iÃ§eriÄŸine gÃ¶z atabilirsiniz.

Åimdi, Google Colab'in Ã§alÄ±ÅŸma dizinini (working directory) kontrol edelim:

```python
!pwd
# /content
```

Daha sonra Python'un yerleÅŸik (built-in) kÃ¼tÃ¼phanelerinden olan `os` kÃ¼tÃ¼phanesini kullanarak `project` isimli klasÃ¶rÃ¼ Drive'da yaratalÄ±m.

```python
path = "./gdrive/MyDrive/project"
os.mkdir(path)
```

ArtÄ±k `project` klasÃ¶rÃ¼ne yarattÄ±ÄŸÄ±mÄ±za gÃ¶re, geÃ§erli Ã§alÄ±ÅŸma dizinini (current working directory) bu klasÃ¶r olarak deÄŸiÅŸtirelim:

```python
os.chdir('./gdrive/MyDrive/project')
```

ArtÄ±k gerÃ§ekleÅŸtireceÄŸimiz tÃ¼m iÅŸlemler bu dizin altÄ±nda yapÄ±lacak, kaydedilecek tÃ¼m dosyalar bu dizin altÄ±nda kaydedilecektir.

# Veri KÃ¼mesi

Bilgisayar kullanÄ±mÄ±yla hastalÄ±klarÄ±n otomatik tespiti Ã¶nemli ancak henÃ¼z keÅŸfedilmemiÅŸ bir araÅŸtÄ±rma alanÄ±dÄ±r. Bu tÃ¼r yenilikler tÃ¼m dÃ¼nyada tÄ±bbi uygulamalarÄ± iyileÅŸtirebilir ve saÄŸlÄ±k bakÄ±m sistemlerini iyileÅŸtirebilir. Bununla birlikte, tÄ±bbi gÃ¶rÃ¼ntÃ¼leri iÃ§eren veri kÃ¼meleri neredeyse hiÃ§ mevcut deÄŸildir, bu da yaklaÅŸÄ±mlarÄ±n tekrarlanabilirliÄŸini ve karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±nÄ± neredeyse imkansÄ±z hale getirmektedir.

Bu nedenle burada gastrointestinal (GI) sistemin iÃ§erisinden gÃ¶rÃ¼ntÃ¼ler iÃ§eren bir veri kÃ¼mesi olan Kvasir'in ikinci versiyonunu (`kvasir-dataset-v2`) kullanÄ±yoruz. Bu veri

Kvasir veri kÃ¼mesi yaklaÅŸÄ±k 2.3GB bÃ¼yÃ¼klÃ¼ÄŸÃ¼ndedir ve yalnÄ±zca araÅŸtÄ±rma ve eÄŸitim amaÃ§lÄ± kullanÄ±lmak suretiyle Ã¼cretsizdir: https://datasets.simula.no/kvasir/

Veri kÃ¼mesi, her biri 1.000 gÃ¶rÃ¼ntÃ¼ye sahip olan 8 sÄ±nÄ±ftan, yani toplam 8.000 gÃ¶rÃ¼ntÃ¼den oluÅŸmaktadÄ±r.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/kvasir_v2_examples.png?raw=true)

Bu sÄ±nÄ±flar patolojik bulgular (Ã¶zofajit, polipler, Ã¼lseratif kolit), anatomik iÅŸaretler (z-Ã§izgisi, pilor, Ã§ekum) ve normal ve dÃ¼zenli bulgular (normal kolon mukozasÄ±, dÄ±ÅŸkÄ±) ve polip Ã§Ä±karma vakalarÄ±ndan (boyalÄ± ve kaldÄ±rÄ±lmÄ±ÅŸ polipler, boyalÄ± rezeksiyon kenarlarÄ±) oluÅŸmaktadÄ±r

JPEG gÃ¶rÃ¼ntÃ¼leri ait olduklarÄ± sÄ±nÄ±fa gÃ¶re adlandÄ±rÄ±lan ayrÄ± klasÃ¶rlerde saklanmaktadÄ±r.

Veri seti, $720 \times 576$'dan $1920 \times 1072$ piksele kadar farklÄ± Ã§Ã¶zÃ¼nÃ¼rlÃ¼kteki gÃ¶rÃ¼ntÃ¼lerden oluÅŸur ve iÃ§eriÄŸe gÃ¶re adlandÄ±rÄ±lmÄ±ÅŸ ayrÄ± klasÃ¶rlerde sÄ±ralanacak ÅŸekilde dÃ¼zenlenmiÅŸtir.

Åimdi yukarÄ±daki websayfasÄ±nda bulunan ve gÃ¶rÃ¼ntÃ¼leri iÃ§eren `kvasir-dataset-v2.zip` isimli zip dosyasÄ±nÄ± indirelim:

```python
!wget https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip

# --2023-10-04 08:44:25--  https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip
# Resolving datasets.simula.no (datasets.simula.no)... 128.39.36.14
# Connecting to datasets.simula.no (datasets.simula.no)|128.39.36.14|:443... connected.
# HTTP request sent, awaiting response... 200 OK
# Length: 2489312085 (2.3G) [application/zip]
# Saving to: â€˜kvasir-dataset-v2.zipâ€™
# 
# kvasir-dataset-v2.z 100%[===================>]   2.32G  14.4MB/s    in 2m 34s  
# 
# 2023-10-04 08:47:00 (15.4 MB/s) - â€˜kvasir-dataset-v2.zipâ€™ saved [2489312085/2489312085]
```

Ä°ndirme tamamlandÄ±ktan sonra, bu zip dosyasÄ±nÄ± unzip'leyelim ve gÃ¶rÃ¼ntÃ¼ verilerimizi elde edelim:

```python
!unzip kvasir-dataset-v2.zip
```

Bu iÅŸlemden sonra `project` klasÃ¶rÃ¼nÃ¼n altÄ±nda `kvasir-dataset-v2` isimli yeni bir klasÃ¶r oluÅŸacaktÄ±r.

zip dosyasÄ±yla iÅŸimiz bittiÄŸi iÃ§in yer kaplamamasÄ± iÃ§in silelim:

```python
!rm -rf kvasir-dataset-v2.zip
```

Daha sonra, `os` kÃ¼tÃ¼phanesini kullanarak, kolaylÄ±k olmasÄ± aÃ§Ä±sÄ±ndan `kvasir-dataset-v2` isimli dosyayÄ± `image_data` olarak yeniden isimlendirelim:

```python
os.rename('kvasir-dataset-v2', 'image_data')
```

Son durumda gÃ¶rÃ¼ntÃ¼lerden oluÅŸan veri kÃ¼mesi yapÄ±sÄ± ÅŸu ÅŸekilde gÃ¶rÃ¼necektir:

```
image_data/dyed-lifted-polyps/0a7bdce4-ac0d-44ef-93ee-92dfc8fe0b81.jpg
image_data/dyed-lifted-polyps/0a7ece5b-caaa-496e-9e6e-0c7eab171527.jpg
...
image_data/dyed-resection-margins/0a0b455d-d3dd-4be4-a6a3-90f81d8c8f36.jpg
image_data/dyed-resection-margins/0a2a2f35-c798-447c-a883-8f2f448bfe07.jpg
...
image_data/ulcerative-colitis/00a436bc-67ee-4a43-b1a7-25130a2d4e72.jpg
image_data/ulcerative-colitis/cat/0aacb7fa-19fb-4bd6-9a43-3c0a246e7a58.jpg
```

Bu Ã¶zel (custom) veri kÃ¼mesini HuggingFace ortamÄ±na yÃ¼klemek iÃ§in bir veri kÃ¼mesinin yapÄ±sÄ±nÄ± (structure) ve iÃ§eriÄŸini (content) deÄŸiÅŸtirmek iÃ§in birÃ§ok araÃ§ saÄŸlayan Hugging Face'in `datasets`  modÃ¼lÃ¼ndeki `load_dataset`  fonksiyonunu kullanabilirsiniz. Bu fonksiyon ya `Dataset` ya da `DatasetDict` dÃ¶ndÃ¼recektir:

```python
full_dataset = load_dataset("imagefolder", data_dir="./image_data", split="train")
full_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 8000
# })
```

Veri kÃ¼mesinde 8000 satÄ±r olduÄŸunu (Ã§Ã¼nkÃ¼ 8000 gÃ¶rÃ¼ntÃ¼ var) ve gÃ¶rÃ¼ntÃ¼lerin etiketlerinin (`label`) otomatik oluÅŸturulduÄŸunu kolaylÄ±kla gÃ¶rebilirsiniz.

```python
full_dataset.features
# {'image': Image(decode=True, id=None),
#  'label': ClassLabel(names=['dyed-lifted-polyps', 'dyed-resection-margins', 'esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis'], id=None)}
```

Bu bir sÃ¶zlÃ¼ktÃ¼r (dictionary). `label` anahtarÄ±nÄ± kullanarak kolaylÄ±kla etiketleri (labels) Ã§ekebilirsiniz:

```python
labels = full_dataset.features['label']
labels
# ClassLabel(names=['dyed-lifted-polyps', 'dyed-resection-margins', 'esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis'], id=None)
```

Spesifik bir gÃ¶rÃ¼ntÃ¼ye ulaÅŸmak da oldukÃ§a kolaydÄ±r. YapmanÄ±z gereken bir indeks kullanmaktÄ±r. Åimdi veri kÃ¼mesindeki ilk gÃ¶rÃ¼ntÃ¼ye ulaÅŸalÄ±m:

```python
full_dataset[0]
# {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=720x576>,
# 'label': 0}
```

GÃ¶rÃ¼ldÃ¼ÄŸÃ¼ Ã¼zere, etiketlere otomatik olarak tamsayÄ± atanmÄ±ÅŸtÄ±r. Burada `0` tamsayÄ±sÄ±na sahip etiketin ismi `dyed-lifted-polyps`'dÄ±r.

Bu gÃ¶rÃ¼ntÃ¼nÃ¼nÃ¼n moduna ve etiketine de kolaylÄ±kla eriÅŸilebilir:

```python
full_dataset[0]['image'].mode
# 'RGB
```

```python
full_dataset[0]['label'], labels.names[full_dataset[0]['label']]
# (0, 'dyed-lifted-polyps')
```

Åimdi etiketler ve bu etiketlere ait id'leri (numaralarÄ±) iÃ§erisinde tutan iki sÃ¶zlÃ¼k (dictionary) oluÅŸturalÄ±m:

```python
id2label = {str(i): label for i, label in enumerate(labels.names)}
print(id2label)
# {'0': 'dyed-lifted-polyps', '1': 'dyed-resection-margins', '2': 'esophagitis', '3': 'normal-cecum', '4': 'normal-pylorus', '5': 'normal-z-line', '6': 'polyps', '7': 'ulcerative-colitis'}

label2id = {v: k for k, v in id2label.items()}
print(label2id)
# {'dyed-lifted-polyps': '0', 'dyed-resection-margins': '1', 'esophagitis': '2', 'normal-cecum': '3', 'normal-pylorus': '4', 'normal-z-line': '5', 'polyps': '6', 'ulcerative-colitis': '7'}
```

Bu iki sÃ¶zlÃ¼ÄŸÃ¼ daha sonra kullanacaÄŸÄ±z.

`datasets` modÃ¼lÃ¼nde bulunan `Image` fonksiyonu, bir gÃ¶rÃ¼ntÃ¼ nesnesini dÃ¶ndÃ¼rmek iÃ§in `image` sÃ¼tunundaki verilerin kodunu otomatik olarak Ã§Ã¶zer. Åimdi gÃ¶rselin ne olduÄŸunu gÃ¶rmek iÃ§in `image` sÃ¼tununu Ã§aÄŸÄ±rmayÄ± deneyiniz:

```python
full_dataset[0]["image"]
```

![](https://raw.githubusercontent.com/mmuratarat/turkish/049aed9bc3e11c206d7c3b90f644162d288174e5/_posts/images/ksavir_vit_image0.png)

```python
full_dataset[1000]["image"]
```

![](https://raw.githubusercontent.com/mmuratarat/turkish/049aed9bc3e11c206d7c3b90f644162d288174e5/_posts/images/ksavir_vit_image1000.png)


# Model

Burada gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma (image classification) gerÃ§ekleÅŸtireceÄŸiz. Veri kÃ¼mesinde 8 farklÄ± sÄ±nÄ±f var. Bu nedenle Ã§ok-sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma (multi-class classification) problemi ile karÅŸÄ± karÅŸÄ±yayÄ±z. 

Ã‡ok-sÄ±nÄ±flÄ± gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma problemi iÃ§in Ã¶nceden-eÄŸitilmiÅŸ (pre-trained) bir modele ihtiyacÄ±mÄ±z var. HuggingFace Hub'da gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma iÃ§in oldukÃ§a fazla Ã¶nceden-eÄŸitilmiÅŸ model bulabilirsiniz - https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads&library=pytorch

GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma iÃ§in transfer Ã¶ÄŸrenmenin arkasÄ±ndaki sezgi, eÄŸer bir model yeterince geniÅŸ ve genel bir veri kÃ¼mesi Ã¼zerinde eÄŸitilirse, bu model etkili bir ÅŸekilde gÃ¶rsel dÃ¼nyanÄ±n genel bir modeli olarak hizmet edebilir. Daha sonra bÃ¼yÃ¼k bir modeli bÃ¼yÃ¼k bir veri kÃ¼mesi Ã¼zerinde eÄŸiterek sÄ±fÄ±rdan baÅŸlamanÄ±za gerek kalmadan bu Ã¶ÄŸrenilen Ã¶zellik haritalarÄ±ndan (feature maps) yararlanabilirsiniz.

BirÃ§ok gÃ¶revde bu yaklaÅŸÄ±m, hedeflenen verileri kullanarak bir modeli sÄ±fÄ±rdan eÄŸitmekten daha iyi sonuÃ§lar vermiÅŸtir.

ArtÄ±k ihtiyaÃ§larÄ±mÄ±za uyacak ÅŸekilde ince ayar yapacaÄŸÄ±mÄ±z temel modelimizi seÃ§ebiliriz.

Burada Visual Transformer (ViT) denilen bir gÃ¶rsel transformer modelini kullanacaÄŸÄ±z (https://huggingface.co/google/vit-base-patch16-224). 

ViT, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" isimli makalede 2021 yÄ±lÄ±nda Dosovitskiy ve arkadaÅŸlarÄ± tarafÄ±ndan tanÄ±tÄ±lmÄ±ÅŸtÄ±r. ViT modeli ImageNet-21k (14 milyon gÃ¶rÃ¼ntÃ¼, 21.843 sÄ±nÄ±f) veri kÃ¼mesi Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸ ve ImageNet 2012 (1 milyon gÃ¶rÃ¼ntÃ¼, 1.000 sÄ±nÄ±f) veri kÃ¼mesi Ã¼zerinde ince ayar Ã§ekilmiÅŸtir. Girdi gÃ¶rÃ¼ntÃ¼leri  $224 x 224$ Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸe sahiptir. Genel olarak Ã¼Ã§ tÃ¼r ViT modeli vardÄ±r:

* ViT-base: 12 katmana, 768 gizli boyuta ve toplam 86M parametreye sahiptir.
* ViT-large: 24 katmana, 1024 gizli boyuta ve toplam 307M parametreye sahiptir.
* ViT-huge: 32 katmanÄ±, 1280 gizli boyutu ve toplam 632M parametresi vardÄ±r.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/Screenshot%202023-10-04%20at%205.17.43%20PM.png?raw=true)

Bu tutorial iÃ§in Hugging Face'te bulunan [the google/vit-base-patch16-224-in21k model](https://huggingface.co/google/vit-base-patch16-224-in21k) modelini kullanacaÄŸÄ±z.

Ä°lk olarak biraz veri temizliÄŸi yapalÄ±m ve RGB olmayan (tek kanallÄ±, gri tonlamalÄ±) gÃ¶rÃ¼ntÃ¼leri veri kÃ¼mesinden kaldÄ±ralÄ±m:

```python
# Remove from dataset images which are non-RGB (single-channel, grayscale)
condition = lambda data: data['image'].mode == 'RGB'
full_dataset = full_dataset.filter(condition)
full_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 8000
# })
```

GÃ¶rÃ¼ntÃ¼lerin hepsi Ã¼Ã§ kanallÄ±, yani RGB modundadÄ±r. Bu nedenle herhangi bir filtreleme iÅŸlemi gerÃ§ekleÅŸmemiÅŸtir.

# Veri kÃ¼mesini eÄŸitim/test olarak parÃ§alamak

Modeli seÃ§tiÄŸimizde gÃ¶re ilk olarak yapmamÄ±z gereken veri kÃ¼mesini eÄŸitim (train) ve test olacak ÅŸekilde ikiye parÃ§alamak. Bunun iÃ§in `train_test_split()` fonksiyonunu kullanabilir ve parÃ§alanmanÄ±n (splitting) boyutunu belirlemek iÃ§in `test_size` parametresini belirtebilirsiniz. Burada test kÃ¼mesinin bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ belirlemek iÃ§in %15 kullandÄ±k, yani, 6800 gÃ¶rÃ¼ntÃ¼ modeli eÄŸitmek iÃ§in, geri kalan 1200 gÃ¶rÃ¼ntÃ¼ modeli test etmek iÃ§in kullanÄ±lacaktÄ±r.

```python
dataset = full_dataset.shuffle().train_test_split(test_size=0.15, stratify_by_column = 'label')
dataset
# DatasetDict({
#     train: Dataset({
#         features: ['image', 'label'],
#         num_rows: 6800
#     })
#     test: Dataset({
#         features: ['image', 'label'],
#         num_rows: 1200
#     })
# })
```

KolaylÄ±kla anlaÅŸÄ±lacaÄŸÄ± Ã¼zere bir DatasetDict nesnesine sahibiz. Bu bir sÃ¶zlÃ¼ktÃ¼r. AnahtarlarÄ± `train` ve `test`'tir. Bu anahtarlardaki deÄŸerleri (yani veri kÃ¼melerini) ayrÄ± ayrÄ± deÄŸiÅŸkenlere atayalÄ±m:

```python
train_dataset = dataset["train"]
train_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 6800
# })
```

```python
test_dataset = dataset["test"]
test_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 1200
# })
```

# Ã–znitelik Ã‡Ä±karÄ±cÄ±

Vision Transformer modeli temel olarak iki Ã¶nemli bileÅŸenden oluÅŸur: bir sÄ±nÄ±flandÄ±rÄ±cÄ± (classifier) ve bir Ã¶znitelik Ã§Ä±karÄ±cÄ± (feature extractor).

ViT modelini kullanarak sÄ±nÄ±flandÄ±rma gerÃ§ekleÅŸtirmeden Ã¶nce Ã–znitelik Ã‡Ä±karÄ±cÄ± (feature extractor) adÄ± verilen bir iÅŸlem gerÃ§ekleÅŸtirmeliyiz. 

Ã–znitelik Ã§Ä±karsama, ses veya gÃ¶rÃ¼ntÃ¼ modelleri iÃ§in girdi Ã¶zniteliklerinin (input features) hazÄ±rlanmasÄ±ndan sorumludur. Bu Ã¶znitelik Ã§Ä±karsama adÄ±mÄ±nÄ±, DoÄŸal Dil Ä°ÅŸleme (Natural Language Processing) gÃ¶revlerindeki Token'laÅŸtÄ±rma (Tokenizing) adÄ±mÄ± olarak dÃ¼ÅŸÃ¼nebilirsiniz.

Ã–znitelik Ã§Ä±karsama, elimizdeki gÃ¶rÃ¼ntÃ¼leri normalleÅŸtirmek (normalizing), yeniden boyutlandÄ±rmak (resizing) ve yeniden Ã¶lÃ§eklendirmek (rescaling) Ã¼zere, gÃ¶rÃ¼ntÃ¼lerin "piksel deÄŸerlerinin" tensÃ¶rlerine Ã¶n-iÅŸleme gerÃ§ekleÅŸtirmek iÃ§in kullanÄ±lÄ±r. 

ViT modeline ait Feature Extractor'Ä± Hugging Face Transformers kÃ¼tÃ¼phanesinden ÅŸu ÅŸekilde baÅŸlatÄ±yoruz:

```python
# modeli iÃ§e aktar
model_name = 'google/vit-base-patch16-224-in21k'
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
feature_extractor
# ViTFeatureExtractor {
#   "do_normalize": true,
#   "do_rescale": true,
#   "do_resize": true,
#   "image_mean": [
#     0.5,
#     0.5,
#     0.5
#   ],
#   "image_processor_type": "ViTFeatureExtractor",
#   "image_std": [
#     0.5,
#     0.5,
#     0.5
#   ],
#   "resample": 2,
#   "rescale_factor": 0.00392156862745098,
#   "size": {
#     "height": 224,
#     "width": 224
#   }
# }
```

Ã–znitelik Ã§Ä±karÄ±cÄ±ya ait yapÄ±landÄ±rma (configuration), normalleÅŸtirme, Ã¶lÃ§ekleme ve yeniden boyutlandÄ±rmanÄ±n `true` olarak ayarlandÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.

SÄ±rasÄ±yla `image_mean` ve `image_std`de saklanan ortalama ve standart sapma deÄŸerleri kullanÄ±larak Ã¼Ã§ renk kanalÄ±nda (RGB) normalleÅŸtirme gerÃ§ekleÅŸtirilir.

Ã‡Ä±ktÄ± boyutu `size` anahtarÄ± ile $224 \times 224$ piksel olarak ayarlanÄ±r.

Bir gÃ¶rÃ¼ntÃ¼yÃ¼ Ã¶znitelik Ã§Ä±karÄ±cÄ±yla iÅŸlemeyi tek bir gÃ¶rÃ¼ntÃ¼ Ã¼zerinde ÅŸu ÅŸekilde gerÃ§ekleÅŸtiririz:

```python
example = feature_extractor(train_dataset[0]['image'], return_tensors='pt')
example
# {'pixel_values': tensor([[[[-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9686],
#          [-0.9686, -0.9608, -0.9608,  ..., -0.9608, -0.9686, -0.9686],
#          [-0.9686, -0.9608, -0.9686,  ..., -0.9686, -0.9686, -0.9765],
#          ...,
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843]],
#
#         [[-0.9608, -0.9608, -0.9529,  ..., -0.9608, -0.9608, -0.9686],
#          [-0.9686, -0.9608, -0.9608,  ..., -0.9608, -0.9686, -0.9686],
#          [-0.9686, -0.9608, -0.9686,  ..., -0.9686, -0.9686, -0.9686],
#          ...,
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843]],
#
#         [[-0.9686, -0.9608, -0.9608,  ..., -0.9686, -0.9608, -0.9765],
#          [-0.9686, -0.9686, -0.9686,  ..., -0.9686, -0.9686, -0.9686],
#          [-0.9686, -0.9765, -0.9765,  ..., -0.9686, -0.9686, -0.9686],
#          ...,
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9922, -0.9922, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9922, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9922, -0.9843, -0.9843]]]])}
```

OluÅŸacak tensÃ¶rÃ¼n boyutu ÅŸu ÅŸekildedir:

```python
example['pixel_values'].shape
```

KolaylÄ±kla anlaÅŸÄ±lacaÄŸÄ± Ã¼zere, Ã¶n iÅŸleme adÄ±mÄ±ndan sonra 4 boyutlu bir tensÃ¶r elde ediliyor. Burada ilk boyut (dimension) yÄ±ÄŸÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ (batch size), ikinci boyut gÃ¶rÃ¼ntÃ¼lerdeki kanal sayÄ±sÄ±nÄ± (number of channels, RGB gÃ¶rÃ¼ntÃ¼ler ile Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ±z iÃ§in Ã¼Ã§ kanal var), Ã¼Ã§Ã¼ncÃ¼ ve dÃ¶ndÃ¼ncÃ¼ boyutlar, sÄ±rasÄ±yla gÃ¶rÃ¼ntÃ¼lerin yÃ¼ksekliÄŸini (height) ve geniÅŸliÄŸini (width) temsil etmektedir.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/image3.jpeg?raw=true)

Burada not edilmesi gereken diÄŸer bir durum `pixel_values` anahtarÄ±nÄ±n sahip olduÄŸu deÄŸer, modelin beklediÄŸi temel girdi olmasÄ±dÄ±r.

Bu Ã¶n iÅŸleme adÄ±mÄ±nÄ± **tÃ¼m veri kÃ¼mesine** daha verimli bir ÅŸekilde uygulamak iÃ§in, `preprocess` adÄ± verilen bir fonksiyon oluÅŸturalÄ±m ve dÃ¶nÃ¼ÅŸÃ¼mleri `map` yÃ¶ntemini kullanarak gerÃ§ekleÅŸtirelim:


```python
def preprocess(examples):
    # take a list of PIL images and turn them to pixel values
    inputs = feature_extractor(examples['image'], return_tensors='pt')
    # include the labels
    inputs['label'] = examples['label']
    return inputs

# apply to train-test datasets
prepared_train = train_dataset.map(preprocess, batched=True)
prepared_test = test_dataset.map(preprocess, batched=True)

prepared_train
# Dataset({
#     features: ['image', 'label', 'pixel_values'],
#     num_rows: 6800
# })

prepared_test
# Dataset({
#     features: ['image', 'label', 'pixel_values'],
#     num_rows: 1200
# })
```

KolaylÄ±kla gÃ¶rÃ¼lebileceÄŸi Ã¼zere artÄ±k eÄŸitim ve test kÃ¼melerinde artÄ±k `'pixel_values'` isimli yeni bir Ã¶zniteliÄŸe sahibiz.

**EK NOT**

Ã–n-iÅŸleme (pre-processing) Ã§ok zaman alabilir. Ã–zellikle not defterinin (notebook) tamamÄ±nÄ± tekrar tekrar Ã§alÄ±ÅŸtÄ±rmanÄ±z gerektiÄŸinde.

Bunu Ã¶nlemek iÃ§in veri kÃ¼melerinizi (eÄŸitim ve test veri kÃ¼meleri) Ã¶n-iÅŸleme tabi tuttuktan sonra diske kaydedin ve tekrar kullanmanÄ±z gerektiÄŸinde bu Ã¶n-iÅŸlenmiÅŸ kÃ¼meleri tekrar yÃ¼kleyin.

Bu nedenle ilk olarak, Ã¶n-iÅŸlemeden geÃ§irilmiÅŸ eÄŸitim ve test kÃ¼melerinin saklanacaÄŸÄ± klasÃ¶rleri Ã§alÄ±ÅŸma dizinimizde (yani `./model`) yaratalÄ±m. Burada, eÄŸitim ve test kÃ¼meleri iÃ§in ayrÄ± dosyalar oluÅŸturuyoruz Ã§Ã¼nkÃ¼ Hugging Face bu veri kÃ¼melerini shard'lara ayÄ±rdÄ±ktan sonra diske kaydetme iÅŸlemi yapmaktadÄ±r:

```python
os.makedirs('./prepared_datasets/train')
os.makedirs('./prepared_datasets/test')
prepared_train.save_to_disk("./prepared_datasets/train")
prepared_test.save_to_disk("./prepared_datasets/test")
```

ArtÄ±k ihtiyacÄ±mÄ±z olduÄŸunda kaydettiÄŸimiz bu Ã¶n-iÅŸlenmiÅŸ veri kÃ¼melerini tekrar geri yÃ¼kleyip iÅŸlerimize devam edebiliriz:

```python
prepared_train = load_from_disk("./prepared_datasets/train")
prepared_test = load_from_disk("./prepared_datasets/test")
```

# ViT'i yÃ¼klemek

Elimizdeki gÃ¶rÃ¼ntÃ¼leri kullanacaÄŸÄ±mÄ±z modelin istediÄŸi uygun formata biÃ§imlendirdikten sonra, bir sonraki adÄ±m ViT'yi indirip baÅŸlatmaktÄ±r (initialize).

Burada da, Ã¶znitelik Ã§Ä±karÄ±cÄ±yÄ± (feature extractor) yÃ¼klemek (load) iÃ§in kullandÄ±ÄŸÄ±mÄ±z `from_pretrained` yÃ¶ntemiyle Hugging Face Transformers'Ä± kullanÄ±yoruz.
