---
layout: post
title: "Ã–nceden EÄŸitilmiÅŸ Visual Transformer (ViT) modeline Ä°nce-Ayar Ã‡ekmek"
author: "MMA"
comments: true
---

SÄ±fÄ±rdan eÄŸitim (training from scratch), bir modelin tamamen yeni bir gÃ¶rev iÃ§in baÅŸtan sona eÄŸitilmesini iÃ§erir. Bu, genellikle bÃ¼yÃ¼k veri setleri ve yÃ¼ksek hesaplama gÃ¼cÃ¼ (computation power) gerektirir. AyrÄ±ca, eÄŸitim sÃ¼reci genellikle gÃ¼nler veya haftalar sÃ¼rebilir. Bu yÃ¶ntem genellikle Ã¶zel bir gÃ¶rev veya dil modeli oluÅŸturmak isteyen araÅŸtÄ±rmacÄ±lar ve bÃ¼yÃ¼k ÅŸirketler tarafÄ±ndan kullanÄ±lÄ±r.

Ancak, bu iÅŸi hobi olarak yapan biri veya bir Ã¶ÄŸrenci iÃ§in bir modeli sÄ±fÄ±rdan oluÅŸturmak o kadar kolay deÄŸildir. BÃ¼yÃ¼k veri ve yÃ¼ksek hesaplama gÃ¼cÃ¼nÃ¼n yanÄ±nda, aynÄ± zamanda oluÅŸturulacak modelin mimarisini (architecture) belirleme ve bu modele ait hiperparametreleri ayarlama (hyperparameter tuning) sÃ¼reci de zorludur. 

Bu sebeple, transfer Ã¶ÄŸrenme (transfer learning) adÄ± verilen bir konsept literatÃ¼rde yerini almÄ±ÅŸtÄ±r. 

"Ã–ÄŸrenilenleri" bir problemden yeni ve farklÄ± bir probleme uyarlamak Transfer Ã–ÄŸrenme fikrini temsil eder. Bir insanÄ±n Ã¶ÄŸrenmesi bÃ¼yÃ¼k Ã¶lÃ§Ã¼de bu Ã¶ÄŸrenme yaklaÅŸÄ±mÄ±na dayanmaktadÄ±r. Transfer Ã¶ÄŸrenimi sayesinde Java Ã¶ÄŸrenmek size oldukÃ§a kolay gelebilir Ã§Ã¼nkÃ¼ Ã¶ÄŸrenme sÃ¼recine girildiÄŸinde zaten programlama kavramlarÄ±nÄ± ve Python sÃ¶zdizimini anlÄ±yorsunuzdur.

AynÄ± mantÄ±k derin Ã¶ÄŸrenme (deep learning) iÃ§in de geÃ§erlidir. Transfer Ã¶ÄŸrenme, genellikle Ã¶nceden-eÄŸitilmiÅŸ (pre-trained) bir modelin (Ã¶rneÄŸin, Hugging Face tarafÄ±ndan saÄŸlanan bir dil modeli) Ã¶zel bir gÃ¶rev veya veri kÃ¼mesine uyarlanmasÄ±dÄ±r. DiÄŸer bir deyiÅŸle, Ã¶nceden eÄŸitilmiÅŸ bir modelin aÄŸÄ±rlÄ±klarÄ± yeni veriler Ã¼zerinde eÄŸitilir. BÃ¶ylelikle, Ã¶nceden eÄŸitilmiÅŸ model yeni bir gÃ¶rev iÃ§in hazÄ±r hale gelir. 

Ã–nceden eÄŸitilmiÅŸ bir model kullanmanÄ±n Ã¶nemli faydalarÄ± vardÄ±r. Hesaplama maliyetlerini ve karbon ayak izinizi azaltÄ±r ve sÄ±fÄ±rdan eÄŸitim gerÃ§ekleÅŸtirmenize gerek kalmadan son teknoloji Ã¼rÃ¼nÃ¼ modelleri kullanmanÄ±za olanak tanÄ±r

ğŸ¤— Hugging Face'in `transformers` kÃ¼tÃ¼phanesi Ã§ok Ã§eÅŸitli gÃ¶revler iÃ§in (Ã¶rneÄŸin, doÄŸal dil iÅŸleme ve bilgisayarlÄ± gÃ¶rÃ¼) Ã¶nceden eÄŸitilmiÅŸ binlerce modele eriÅŸim saÄŸlar (https://huggingface.co/models). Ã–nceden eÄŸitilmiÅŸ bir model kullandÄ±ÄŸÄ±nÄ±zda, onu gÃ¶revinize Ã¶zel bir veri kÃ¼mesi Ã¼zerinde eÄŸitirsiniz. Bu, inanÄ±lmaz derecede gÃ¼Ã§lÃ¼ bir eÄŸitim tekniÄŸi olan ince-ayar (fine-tuning) olarak bilinir.

Transformer-tabanlÄ± modeller genellikle gÃ¶revden baÄŸÄ±msÄ±z gÃ¶vde (task-independent body) ve gÃ¶reve Ã¶zel kafa (task-specific head) olarak ikiye ayrÄ±lÄ±r. GÃ¶revden baÄŸÄ±msÄ±z kÄ±sÄ±m genellikle Hugging Face tarafÄ±ndan saÄŸlanan aÄŸÄ±rlÄ±klara (weights) sahiptir. Bu kÄ±sÄ±mdaki aÄŸÄ±rlÄ±klar dondurulmuÅŸtur ve herhangi bir gÃ¼ncellemeye (updates) sahip olmazlar. GÃ¶reve Ã¶zel kafa'da, elinizdeki gÃ¶rev iÃ§in ihtiyacÄ±nÄ±z kadar nÃ¶ron oluÅŸturulur ve sadece bu katmanda eÄŸitim Ã¶zel veri kÃ¼meniz kullanÄ±larak gerÃ§ekleÅŸtirilir.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/fine_tuning_example.png?raw=true)

Ä°nce-ayar, sinir aÄŸÄ±nÄ±n tamamÄ±nda veya yalnÄ±zca katmanlarÄ±nÄ±n bir alt kÃ¼mesinde yapÄ±labilir; bu durumda, ince ayarÄ± yapÄ±lmayan katmanlar "dondurulur (frozen)" (geri yayÄ±lÄ±m (backpropagation) adÄ±mÄ± sÄ±rasÄ±nda gÃ¼ncellenmez).

Ä°ÅŸte, bu tutorial'da Ã¶zel bir veri kÃ¼mesi (a custom dataset) iÃ§in Ã¶nceden eÄŸitilmiÅŸ bir modele ince ayar yapacaksÄ±nÄ±z.

# Google Colab'e GiriÅŸ

Burada gerÃ§ekleÅŸtireceÄŸiniz analizleri GPU'ya sahip bir makinede yapmanÄ±zda fayda var. Ã‡Ã¼nkÃ¼ kullanÄ±lacak ViT modeli ve veri kÃ¼mesi oldukÃ§a bÃ¼yÃ¼k. Bu nedenle modele ince-ayar Ã§ekmek oldukÃ§a zaman alabilir. 

Bu tutorial'da kullanÄ±lacak tÃ¼m Python kÃ¼tÃ¼phanelerini aÅŸaÄŸÄ±daki ÅŸekilde iÃ§e aktarma ile iÅŸe baÅŸlayalÄ±m:

```python
from google.colab import drive
import os
import copy
import numpy as np
import pandas as pd
pd.set_option('display.max_columns', None)
import torch
from datasets import load_dataset, Image, load_from_disk
from transformers import (ViTFeatureExtractor,
                          ViTForImageClassification,
                          TrainingArguments,
                          TrainerCallback,
                          Trainer,
                          AutoModelForImageClassification,
                          AutoFeatureExtractor)
import evaluate
from huggingface_hub import notebook_login, create_repo
```

Tabii ki, bu kÃ¼tÃ¼phaneler kiÅŸisel bilgisayarÄ±nÄ±zda veya Colab ortamÄ±nÄ±zda yÃ¼klÃ¼ deÄŸilse, Ã¶ncelikle bunlarÄ± yÃ¼klemeniz (install) gerekmektedir:

```python
!pip3 install datasets
!pip install transformers
!pip install transformers[torch]
!pip install accelerate -U
!pip install evaluate
!pip install scikit-learn
```

Gerekli kÃ¼tÃ¼phaneler yÃ¼klendikten ve bu kÃ¼tÃ¼phaneler Python ortamÄ±na iÃ§eri aktarÄ±ldÄ±ktan sonra, yapmanÄ±z gereken iÅŸlem Depolama (Storage) alanÄ±nÄ± ayarlamaktÄ±r.

Google Colab'in bir faydasÄ±, Google Drive'Ä±nÄ±za baÄŸlanmanÄ±za olanak saÄŸlamasÄ±dr. BÃ¶ylelikle, elinizdeki veriyi Drive'da barÄ±ndÄ±rÄ±rken, kodlarÄ±nÄ±zÄ± GPU destekli bir Jupyter Not Defterinde Ã§alÄ±ÅŸtÄ±rabilirsiniz.

Åimdi Google Drive'Ä± Colab'a baÄŸlayalÄ±m. Google Drive'Ä±nÄ±zÄ±n tamamÄ±nÄ± Colab'a baÄŸlamak iÃ§in `google.colab` kÃ¼tÃ¼phanesindeki `drive` modÃ¼lÃ¼nÃ¼ kullanabilirsiniz:

```python
drive.mount('/content/gdrive')
```

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/google_drive_colab_permission2.png?raw=true)
![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/google_drive_colab_permission.png?raw=true)

Google HesabÄ±nÄ±za eriÅŸim izni verdikten sonra Drive'a baÄŸlanabilirsiniz.

Drive baÄŸlandÄ±ktan sonra `"Mounted at /content/gdrive"` mesajÄ±nÄ± alÄ±rsÄ±nÄ±z ve dosya gezgini bÃ¶lmesinden Drive'Ä±nÄ±zÄ±n iÃ§eriÄŸine gÃ¶z atabilirsiniz.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/colab_file_browser.png?raw=true)

Åimdi, Google Colab'in Ã§alÄ±ÅŸma dizinini (working directory) kontrol edelim:

```python
!pwd
# /content
```

Daha sonra Python'un yerleÅŸik (built-in) kÃ¼tÃ¼phanelerinden olan `os` kÃ¼tÃ¼phanesini kullanarak `project` isimli klasÃ¶rÃ¼ Drive'da yaratalÄ±m.

```python
path = "./gdrive/MyDrive/project"
os.mkdir(path)
```

ArtÄ±k `project` klasÃ¶rÃ¼ne yarattÄ±ÄŸÄ±mÄ±za gÃ¶re, geÃ§erli Ã§alÄ±ÅŸma dizinini (current working directory) bu klasÃ¶r olarak deÄŸiÅŸtirelim:

```python
os.chdir('./gdrive/MyDrive/project')
```

ArtÄ±k gerÃ§ekleÅŸtireceÄŸimiz tÃ¼m iÅŸlemler bu dizin altÄ±nda yapÄ±lacak, kaydedilecek tÃ¼m dosyalar bu dizin altÄ±nda kaydedilecektir.

# Veri KÃ¼mesi

Bilgisayar kullanÄ±mÄ±yla hastalÄ±klarÄ±n otomatik tespiti Ã¶nemli ancak henÃ¼z keÅŸfedilmemiÅŸ bir araÅŸtÄ±rma alanÄ±dÄ±r. Bu tÃ¼r yenilikler tÃ¼m dÃ¼nyada tÄ±bbi uygulamalarÄ± iyileÅŸtirebilir ve saÄŸlÄ±k bakÄ±m sistemlerini iyileÅŸtirebilir. Bununla birlikte, tÄ±bbi gÃ¶rÃ¼ntÃ¼leri iÃ§eren veri kÃ¼meleri neredeyse hiÃ§ mevcut deÄŸildir, bu da yaklaÅŸÄ±mlarÄ±n tekrarlanabilirliÄŸini ve karÅŸÄ±laÅŸtÄ±rÄ±lmasÄ±nÄ± neredeyse imkansÄ±z hale getirmektedir.

Bu nedenle burada, tÄ±p doktorlarÄ± (deneyimli endoskopistler) tarafÄ±ndan etiketlenmiÅŸ ve doÄŸrulanmÄ±ÅŸ, gastrointestinal (gastrointestinal) sistemin iÃ§inden gÃ¶rÃ¼ntÃ¼lerden oluÅŸan Kvasir veri kÃ¼mesinin 2. versiyonunu (`kvasir-dataset-v2`) kullanacaÄŸÄ±z.

Kvasir veri kÃ¼mesi yaklaÅŸÄ±k 2.3GB bÃ¼yÃ¼klÃ¼ÄŸÃ¼ndedir ve yalnÄ±zca araÅŸtÄ±rma ve eÄŸitim amaÃ§lÄ± kullanÄ±lmak suretiyle Ã¼cretsizdir: https://datasets.simula.no/kvasir/

Veri kÃ¼mesi, her biri 1.000 gÃ¶rÃ¼ntÃ¼ye sahip olan 8 sÄ±nÄ±ftan, yani toplam 8.000 gÃ¶rÃ¼ntÃ¼den oluÅŸmaktadÄ±r.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/kvasir_v2_examples.png?raw=true)

GÃ¶rÃ¼ntÃ¼lerden oluÅŸan bu koleksiyon, Ã¼Ã§ Ã¶nemli anatomik iÅŸareti ve Ã¼Ã§ klinik aÃ§Ä±dan Ã¶nemli bulgu halinde sÄ±nÄ±flandÄ±rÄ±lmÄ±ÅŸtÄ±r. AyrÄ±ca endoskopik polip Ã§Ä±karÄ±lmasÄ±yla ilgili iki kategoride gÃ¶rÃ¼ntÃ¼ iÃ§ermektedir.

Anatomik iÅŸaretler arasÄ±nda z-Ã§izgisi (_z-line_), pilor (_pylorus_), Ã§ekum(_Ã§ecum_) bulunurken patolojik bulgu Ã¶zofajit (_esophagitis_), polipler (_polyps_), Ã¼lseratif kolit (_ulcerative colitis_) iÃ§ermektedir. Ek olarak, lezyonlarÄ±n Ã§Ä±karÄ±lmasÄ±yla ilgili Ã§eÅŸitli gÃ¶rÃ¼ntÃ¼ler de sunulmaktadur; Ã¶rneÄŸin boyalÄ± ve kaldÄ±rÄ±lmÄ±ÅŸ polipler (_dyed and lifted polyps_), boyalÄ± rezeksiyon kenarlarÄ± (_dyed resection margins_).

JPEG gÃ¶rÃ¼ntÃ¼leri ait olduklarÄ± sÄ±nÄ±fa gÃ¶re adlandÄ±rÄ±lan ayrÄ± klasÃ¶rlerde saklanmaktadÄ±r.

Veri seti, $720 \times 576$'dan $1920 \times 1072$ piksele kadar farklÄ± Ã§Ã¶zÃ¼nÃ¼rlÃ¼kteki gÃ¶rÃ¼ntÃ¼lerden oluÅŸur ve iÃ§eriÄŸe gÃ¶re adlandÄ±rÄ±lmÄ±ÅŸ ayrÄ± klasÃ¶rlerde saklanacak ÅŸekilde dÃ¼zenlenmiÅŸtir.

Åimdi yukarÄ±daki websayfasÄ±nda bulunan ve gÃ¶rÃ¼ntÃ¼leri iÃ§eren `kvasir-dataset-v2.zip` isimli zip dosyasÄ±nÄ± `wget` komutu ile `project` dizinine indirelim:

```python
!wget https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip

# --2023-10-04 08:44:25--  https://datasets.simula.no/downloads/kvasir/kvasir-dataset-v2.zip
# Resolving datasets.simula.no (datasets.simula.no)... 128.39.36.14
# Connecting to datasets.simula.no (datasets.simula.no)|128.39.36.14|:443... connected.
# HTTP request sent, awaiting response... 200 OK
# Length: 2489312085 (2.3G) [application/zip]
# Saving to: â€˜kvasir-dataset-v2.zipâ€™
# 
# kvasir-dataset-v2.z 100%[===================>]   2.32G  14.4MB/s    in 2m 34s  
# 
# 2023-10-04 08:47:00 (15.4 MB/s) - â€˜kvasir-dataset-v2.zipâ€™ saved [2489312085/2489312085]
```

Ä°ndirme tamamlandÄ±ktan sonra, bu zip dosyasÄ±nÄ± unzip'leyelim ve gÃ¶rÃ¼ntÃ¼ verilerimizi elde edelim:

```python
!unzip kvasir-dataset-v2.zip
```

Bu iÅŸlemden sonra `project` klasÃ¶rÃ¼nÃ¼n altÄ±nda `kvasir-dataset-v2` isimli yeni bir klasÃ¶r oluÅŸacaktÄ±r.

zip dosyasÄ±yla iÅŸimiz bittiÄŸi iÃ§in yer kaplamamasÄ± iÃ§in `rm` komutu ile silelim:

```python
!rm -rf kvasir-dataset-v2.zip
```

Daha sonra, `os` kÃ¼tÃ¼phanesini kullanarak, kolaylÄ±k olmasÄ± aÃ§Ä±sÄ±ndan `kvasir-dataset-v2` isimli dosyayÄ± `image_data` olarak yeniden isimlendirelim:

```python
os.rename('kvasir-dataset-v2', 'image_data')
```

Son durumda gÃ¶rÃ¼ntÃ¼lerden oluÅŸan veri kÃ¼mesi yapÄ±sÄ± (dataset structure) ÅŸu ÅŸekilde olacaktÄ±r:

```
image_data/dyed-lifted-polyps/0a7bdce4-ac0d-44ef-93ee-92dfc8fe0b81.jpg
image_data/dyed-lifted-polyps/0a7ece5b-caaa-496e-9e6e-0c7eab171527.jpg
...
image_data/dyed-resection-margins/0a0b455d-d3dd-4be4-a6a3-90f81d8c8f36.jpg
image_data/dyed-resection-margins/0a2a2f35-c798-447c-a883-8f2f448bfe07.jpg
...
image_data/ulcerative-colitis/00a436bc-67ee-4a43-b1a7-25130a2d4e72.jpg
image_data/ulcerative-colitis/cat/0aacb7fa-19fb-4bd6-9a43-3c0a246e7a58.jpg
```

Bu Ã¶zel (custom) veri kÃ¼mesini HuggingFace ortamÄ±na yÃ¼klemek Ã¼zere bir veri kÃ¼mesinin yapÄ±sÄ±nÄ± (structure) ve iÃ§eriÄŸini (content) deÄŸiÅŸtirmek iÃ§in birÃ§ok araÃ§ saÄŸlayan Hugging Face'in `datasets`  modÃ¼lÃ¼ndeki `load_dataset`  fonksiyonunu kullanabilirsiniz. Bu fonksiyon ya `Dataset` ya da `DatasetDict` dÃ¶ndÃ¼recektir:

```python
full_dataset = load_dataset("imagefolder", data_dir="./image_data", split="train")
full_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 8000
# })
```

Veri kÃ¼mesinde 8,000 gÃ¶rÃ¼ntÃ¼ olduÄŸunu ve gÃ¶rÃ¼ntÃ¼lerin etiketlerinin (`label`) otomatik oluÅŸturulduÄŸunu kolaylÄ±kla gÃ¶rebilirsiniz.

```python
full_dataset.features
# {'image': Image(decode=True, id=None),
#  'label': ClassLabel(names=['dyed-lifted-polyps', 'dyed-resection-margins', 'esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis'], id=None)}
```

Bu bir sÃ¶zlÃ¼ktÃ¼r (dictionary). `label` anahtarÄ±nÄ± kullanarak kolaylÄ±kla etiketleri (labels) Ã§ekebilirsiniz:

```python
labels = full_dataset.features['label']
labels
# ClassLabel(names=['dyed-lifted-polyps', 'dyed-resection-margins', 'esophagitis', 'normal-cecum', 'normal-pylorus', 'normal-z-line', 'polyps', 'ulcerative-colitis'], id=None)
```

Spesifik bir gÃ¶rÃ¼ntÃ¼ye ulaÅŸmak da oldukÃ§a kolaydÄ±r. YapmanÄ±z gereken bir indeks kullanmaktÄ±r. Åimdi veri kÃ¼mesindeki ilk gÃ¶rÃ¼ntÃ¼ye ulaÅŸalÄ±m:

```python
full_dataset[0]
# {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=720x576>,
# 'label': 0}
```

GÃ¶rÃ¼ldÃ¼ÄŸÃ¼ Ã¼zere, etiketlere otomatik olarak tamsayÄ± atanmÄ±ÅŸtÄ±r. Burada `0` tamsayÄ±sÄ±na sahip etiketin ismi `dyed-lifted-polyps`'dÄ±r.

```python
full_dataset[0]['label'], labels.names[full_dataset[0]['label']]
# (0, 'dyed-lifted-polyps')
```

Bu gÃ¶rÃ¼ntÃ¼nÃ¼nÃ¼n moduna ve etiketine de kolaylÄ±kla eriÅŸilebilir:

```python
full_dataset[0]['image'].mode
# 'RGB
```

Åimdi etiketler ve bu etiketlere ait id'leri (numaralarÄ±) iÃ§erisinde tutan iki sÃ¶zlÃ¼k (dictionary) oluÅŸturalÄ±m:

```python
id2label = {str(i): label for i, label in enumerate(labels.names)}
print(id2label)
# {'0': 'dyed-lifted-polyps', '1': 'dyed-resection-margins', '2': 'esophagitis', '3': 'normal-cecum', '4': 'normal-pylorus', '5': 'normal-z-line', '6': 'polyps', '7': 'ulcerative-colitis'}

label2id = {v: k for k, v in id2label.items()}
print(label2id)
# {'dyed-lifted-polyps': '0', 'dyed-resection-margins': '1', 'esophagitis': '2', 'normal-cecum': '3', 'normal-pylorus': '4', 'normal-z-line': '5', 'polyps': '6', 'ulcerative-colitis': '7'}
```

Bu iki sÃ¶zlÃ¼ÄŸÃ¼ daha sonra kullanacaÄŸÄ±z.

`datasets` modÃ¼lÃ¼nde bulunan `Image` fonksiyonu, bir gÃ¶rÃ¼ntÃ¼ nesnesini dÃ¶ndÃ¼rmek iÃ§in `image` sÃ¼tunundaki verilerin kodunu otomatik olarak Ã§Ã¶zer. Åimdi gÃ¶rselin ne olduÄŸunu gÃ¶rmek iÃ§in `image` sÃ¼tununu Ã§aÄŸÄ±rmayÄ± deneyiniz:

```python
full_dataset[0]["image"]
```

![](https://raw.githubusercontent.com/mmuratarat/turkish/049aed9bc3e11c206d7c3b90f644162d288174e5/_posts/images/ksavir_vit_image0.png)

```python
full_dataset[1000]["image"]
```

![](https://raw.githubusercontent.com/mmuratarat/turkish/049aed9bc3e11c206d7c3b90f644162d288174e5/_posts/images/ksavir_vit_image1000.png)


# Model

Burada gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma (image classification) gÃ¶revini gerÃ§ekleÅŸtireceÄŸiz. Veri kÃ¼mesinde 8 farklÄ± sÄ±nÄ±f var. Bu nedenle Ã§ok-sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma (multi-class classification) problemi ile karÅŸÄ± karÅŸÄ±yayÄ±z. 

Ã‡ok-sÄ±nÄ±flÄ± gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma problemi iÃ§in Ã¶nceden-eÄŸitilmiÅŸ (pre-trained) bir modele ihtiyacÄ±mÄ±z var. HuggingFace Hub'da gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma iÃ§in oldukÃ§a fazla Ã¶nceden-eÄŸitilmiÅŸ model bulabilirsiniz - https://huggingface.co/models?pipeline_tag=image-classification&sort=downloads&library=pytorch

GÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma iÃ§in transfer Ã¶ÄŸrenmenin arkasÄ±ndaki sezgi, eÄŸer bir model yeterince geniÅŸ ve genel bir veri kÃ¼mesi Ã¼zerinde eÄŸitilirse, bu model etkili bir ÅŸekilde gÃ¶rsel dÃ¼nyanÄ±n genel bir modeli olarak hizmet edebilir. Daha sonra bÃ¼yÃ¼k bir modeli bÃ¼yÃ¼k bir veri kÃ¼mesi Ã¼zerinde eÄŸiterek sÄ±fÄ±rdan baÅŸlamanÄ±za gerek kalmadan bu Ã¶ÄŸrenilen Ã¶zellik haritalarÄ±ndan (feature maps) yararlanabilirsiniz.

BirÃ§ok gÃ¶revde kullanabileceÄŸiniz bu yaklaÅŸÄ±m, hedeflenen verileri kullanarak bir modeli sÄ±fÄ±rdan eÄŸitmekten daha iyi sonuÃ§lar vermektedir.

ArtÄ±k ihtiyaÃ§larÄ±mÄ±za uyacak ÅŸekilde ince ayar yapacaÄŸÄ±mÄ±z temel modelimizi seÃ§ebiliriz.

Burada Visual Transformer (ViT) denilen bir gÃ¶rsel transformer modelini kullanacaÄŸÄ±z (https://huggingface.co/google/vit-base-patch16-224). 

ViT, "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" isimli makalede 2021 yÄ±lÄ±nda Dosovitskiy ve arkadaÅŸlarÄ± tarafÄ±ndan tanÄ±tÄ±lmÄ±ÅŸtÄ±r. ViT modeli ImageNet-21k (14 milyon gÃ¶rÃ¼ntÃ¼, 21.843 sÄ±nÄ±f) veri kÃ¼mesi Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸ ve ImageNet 2012 (1 milyon gÃ¶rÃ¼ntÃ¼, 1.000 sÄ±nÄ±f) veri kÃ¼mesi Ã¼zerinde ince ayar Ã§ekilmiÅŸtir. Girdi gÃ¶rÃ¼ntÃ¼leri  $224 x 224$ Ã§Ã¶zÃ¼nÃ¼rlÃ¼ÄŸe sahiptir. Genel olarak Ã¼Ã§ tÃ¼r ViT modeli vardÄ±r:

* ViT-base: 12 katmana, 768 gizli boyuta ve toplam 86M parametreye sahiptir.
* ViT-large: 24 katmana, 1024 gizli boyuta ve toplam 307M parametreye sahiptir.
* ViT-huge: 32 katmanÄ±, 1280 gizli boyutu ve toplam 632M parametresi vardÄ±r.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/vit_architecture.jpg?raw=true)

Bu tutorial iÃ§in Hugging Face'te bulunan [the google/vit-base-patch16-224-in21k model](https://huggingface.co/google/vit-base-patch16-224-in21k) modelini kullanacaÄŸÄ±z.

Ä°lk olarak biraz veri temizliÄŸi yapalÄ±m ve RGB olmayan (tek kanallÄ±, gri tonlamalÄ±) gÃ¶rÃ¼ntÃ¼leri veri kÃ¼mesinden kaldÄ±ralÄ±m:

```python
# Remove from dataset images which are non-RGB (single-channel, grayscale)
condition = lambda data: data['image'].mode == 'RGB'
full_dataset = full_dataset.filter(condition)
full_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 8000
# })
```

GÃ¶rÃ¼ntÃ¼lerin hepsi Ã¼Ã§ kanallÄ±, yani RGB modundadÄ±r. Bu nedenle herhangi bir filtreleme iÅŸlemi gerÃ§ekleÅŸmemiÅŸtir.

# Veri kÃ¼mesini eÄŸitim/test olarak parÃ§alamak

Modeli seÃ§tiÄŸimizde gÃ¶re ilk olarak yapmamÄ±z gereken veri kÃ¼mesini eÄŸitim (train) ve test olacak ÅŸekilde ikiye parÃ§alamak (splitting). Bunun iÃ§in Hugging Face'in `train_test_split()` fonksiyonunu(https://huggingface.co/docs/datasets/v2.14.5/en/package_reference/main_classes#datasets.Dataset.train_test_split) kullanabilir ve parÃ§alanmanÄ±n (splitting) boyutunu belirlemek iÃ§in `test_size` argÃ¼manÄ± belirtebilirsiniz. Burada test kÃ¼mesinin bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ belirlemek iÃ§in `test_size` argÃ¼manÄ±nÄ±n deÄŸeri olarak %15 kullanÄ±yoruz, yani, 6800 gÃ¶rÃ¼ntÃ¼, modeli eÄŸitmek iÃ§in, geri kalan 1200 gÃ¶rÃ¼ntÃ¼, modeli test etmek iÃ§in kullanÄ±lacaktÄ±r.

```python
dataset = full_dataset.shuffle().train_test_split(test_size=0.15, stratify_by_column = 'label')
dataset
# DatasetDict({
#     train: Dataset({
#         features: ['image', 'label'],
#         num_rows: 6800
#     })
#     test: Dataset({
#         features: ['image', 'label'],
#         num_rows: 1200
#     })
# })
```

KolaylÄ±kla anlaÅŸÄ±lacaÄŸÄ± Ã¼zere bir `DatasetDict` nesnesine sahibiz artÄ±k. Bu bir sÃ¶zlÃ¼ktÃ¼r. AnahtarlarÄ± (keys) `train` ve `test`'tir. Bu anahtarlardaki deÄŸerleri (yani, veri kÃ¼melerini) ayrÄ± ayrÄ± deÄŸiÅŸkenlere atayalÄ±m:

```python
train_dataset = dataset["train"]
train_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 6800
# })
```

```python
test_dataset = dataset["test"]
test_dataset
# Dataset({
#     features: ['image', 'label'],
#     num_rows: 1200
# })
```

# Ã–znitelik Ã‡Ä±karÄ±cÄ±

Vision Transformer modeli temel olarak iki Ã¶nemli bileÅŸenden oluÅŸur: bir sÄ±nÄ±flandÄ±rÄ±cÄ± (classifier) ve bir Ã¶znitelik Ã§Ä±karÄ±cÄ± (feature extractor).

ViT modelini kullanarak sÄ±nÄ±flandÄ±rma gerÃ§ekleÅŸtirmeden Ã¶nce **Ã–znitelik Ã‡Ä±karÄ±cÄ±** (Feature Extractor) adÄ± verilen bir iÅŸlem gerÃ§ekleÅŸtirmeliyiz. 

Ã–znitelik Ã§Ä±karsama, ses veya gÃ¶rÃ¼ntÃ¼ modelleri iÃ§in girdi Ã¶zniteliklerinin (input features) hazÄ±rlanmasÄ±ndan sorumludur. Bu Ã¶znitelik Ã§Ä±karsama adÄ±mÄ±nÄ±, DoÄŸal Dil Ä°ÅŸleme (Natural Language Processing) gÃ¶revlerindeki Token'laÅŸtÄ±rma (Tokenizer) adÄ±mÄ± olarak dÃ¼ÅŸÃ¼nebilirsiniz.

Ã–znitelik Ã§Ä±karsama, elimizdeki gÃ¶rÃ¼ntÃ¼leri normalleÅŸtirmek (normalizing), yeniden boyutlandÄ±rmak (resizing) ve yeniden Ã¶lÃ§eklendirmek (rescaling) Ã¼zere, gÃ¶rÃ¼ntÃ¼lerin "piksel deÄŸerlerinin" tensÃ¶rlerine Ã¶n-iÅŸleme gerÃ§ekleÅŸtirmek iÃ§in kullanÄ±lÄ±r. 

ViT modeline ait Feature Extractor'Ä± Hugging Face'in `transformers` kÃ¼tÃ¼phanesinden ÅŸu ÅŸekilde baÅŸlatÄ±yoruz:

```python
# modeli iÃ§e aktar
model_name = 'google/vit-base-patch16-224-in21k'
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)
feature_extractor
# ViTFeatureExtractor {
#   "do_normalize": true,
#   "do_rescale": true,
#   "do_resize": true,
#   "image_mean": [
#     0.5,
#     0.5,
#     0.5
#   ],
#   "image_processor_type": "ViTFeatureExtractor",
#   "image_std": [
#     0.5,
#     0.5,
#     0.5
#   ],
#   "resample": 2,
#   "rescale_factor": 0.00392156862745098,
#   "size": {
#     "height": 224,
#     "width": 224
#   }
# }
```

Ã–znitelik Ã§Ä±karÄ±cÄ±ya ait yapÄ±landÄ±rma (configuration), normalleÅŸtirme, Ã¶lÃ§ekleme ve yeniden boyutlandÄ±rmanÄ±n `true` olarak ayarlandÄ±ÄŸÄ±nÄ± gÃ¶stermektedir.

SÄ±rasÄ±yla `image_mean` ve `image_std`de saklanan ortalama ve standart sapma deÄŸerleri kullanÄ±larak Ã¼Ã§ renk kanalÄ±nda (Red Green Blue - RGB) normalleÅŸtirme gerÃ§ekleÅŸtirilir.

Ã‡Ä±ktÄ± boyutu `size` anahtarÄ± ile $224 \times 224$ piksel olarak ayarlanÄ±r.

Ã–znitelik Ã§Ä±karÄ±cÄ±yla iÅŸlemeyi tek bir gÃ¶rÃ¼ntÃ¼ Ã¼zerinde ÅŸu ÅŸekilde gerÃ§ekleÅŸtiririz:

```python
example = feature_extractor(train_dataset[0]['image'], return_tensors='pt')
example
# {'pixel_values': tensor([[[[-0.9608, -0.9608, -0.9608,  ..., -0.9608, -0.9608, -0.9686],
#          [-0.9686, -0.9608, -0.9608,  ..., -0.9608, -0.9686, -0.9686],
#          [-0.9686, -0.9608, -0.9686,  ..., -0.9686, -0.9686, -0.9765],
#          ...,
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843]],
#
#         [[-0.9608, -0.9608, -0.9529,  ..., -0.9608, -0.9608, -0.9686],
#          [-0.9686, -0.9608, -0.9608,  ..., -0.9608, -0.9686, -0.9686],
#          [-0.9686, -0.9608, -0.9686,  ..., -0.9686, -0.9686, -0.9686],
#          ...,
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9843, -0.9843, -0.9843]],
#
#         [[-0.9686, -0.9608, -0.9608,  ..., -0.9686, -0.9608, -0.9765],
#          [-0.9686, -0.9686, -0.9686,  ..., -0.9686, -0.9686, -0.9686],
#          [-0.9686, -0.9765, -0.9765,  ..., -0.9686, -0.9686, -0.9686],
#          ...,
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9922, -0.9922, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9922, -0.9843, -0.9843],
#          [-0.9843, -0.9843, -0.9843,  ..., -0.9922, -0.9843, -0.9843]]]])}
```

OluÅŸacak tensÃ¶rÃ¼n boyutu ÅŸu ÅŸekildedir:

```python
example['pixel_values'].shape
```

KolaylÄ±kla anlaÅŸÄ±lacaÄŸÄ± Ã¼zere, Ã¶n-iÅŸleme adÄ±mÄ±ndan sonra 4 boyutlu bir tensÃ¶r elde edilmektedir. Burada ilk boyut (dimension) yÄ±ÄŸÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼nÃ¼ (batch size), ikinci boyut gÃ¶rÃ¼ntÃ¼lerdeki kanal sayÄ±sÄ±nÄ± (number of channels, RGB gÃ¶rÃ¼ntÃ¼ler ile Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ±z iÃ§in Ã¼Ã§ kanal var), Ã¼Ã§Ã¼ncÃ¼ ve dÃ¶rdÃ¼ncÃ¼ boyutlar, sÄ±rasÄ±yla gÃ¶rÃ¼ntÃ¼lerin yÃ¼ksekliÄŸini (height) ve geniÅŸliÄŸini (width) temsil etmektedir.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/image3.jpeg?raw=true)

Burada not edilmesi gereken diÄŸer bir durum `pixel_values` anahtarÄ±nÄ±n sahip olduÄŸu deÄŸerin, modelin beklediÄŸi temel girdi olmasÄ±dÄ±r.

Bu Ã¶n-iÅŸleme adÄ±mÄ±nÄ± **tÃ¼m veri kÃ¼mesine** daha verimli bir ÅŸekilde uygulamak iÃ§in, `preprocess` adÄ± verilen bir fonksiyon oluÅŸturalÄ±m ve dÃ¶nÃ¼ÅŸÃ¼mleri `map` yÃ¶ntemini kullanarak gerÃ§ekleÅŸtirelim:

```python
def preprocess(examples):
    # take a list of PIL images and turn them to pixel values
    inputs = feature_extractor(examples['image'], return_tensors='pt')
    # include the labels
    inputs['label'] = examples['label']
    return inputs

# apply to train-test datasets
prepared_train = train_dataset.map(preprocess, batched=True)
prepared_test = test_dataset.map(preprocess, batched=True)

prepared_train
# Dataset({
#     features: ['image', 'label', 'pixel_values'],
#     num_rows: 6800
# })

prepared_test
# Dataset({
#     features: ['image', 'label', 'pixel_values'],
#     num_rows: 1200
# })
```

KolaylÄ±kla gÃ¶rÃ¼lebileceÄŸi Ã¼zere eÄŸitim ve test kÃ¼melerinde artÄ±k `'pixel_values'` isimli yeni bir Ã¶zniteliÄŸe sahibiz.

**EK NOT**

Ã–n-iÅŸleme (pre-processing) Ã§ok zaman alabilir. Ã–zellikle not defterinin (notebook) tamamÄ±nÄ± tekrar tekrar Ã§alÄ±ÅŸtÄ±rmanÄ±z gerektiÄŸinde.

Bunu Ã¶nlemek iÃ§in veri kÃ¼melerinizi (eÄŸitim ve test veri kÃ¼meleri) Ã¶n-iÅŸleme tabi tuttuktan sonra diske kaydedin ve tekrar kullanmanÄ±z gerektiÄŸinde bu Ã¶n-iÅŸlenmiÅŸ kÃ¼meleri tekrar yÃ¼kleyin.

Bu nedenle ilk olarak, Ã¶n-iÅŸlemeden geÃ§irilmiÅŸ eÄŸitim ve test kÃ¼melerinin saklanacaÄŸÄ± klasÃ¶rleri Ã§alÄ±ÅŸma dizinimizde (yani `./model`) yaratalÄ±m. Burada, eÄŸitim ve test kÃ¼meleri iÃ§in ayrÄ± dosyalar oluÅŸturuyoruz Ã§Ã¼nkÃ¼ Hugging Face bu veri kÃ¼melerini shard'lara ayÄ±rdÄ±ktan sonra diske kaydetme iÅŸlemi yapmaktadÄ±r:

```python
os.makedirs('./prepared_datasets/train')
os.makedirs('./prepared_datasets/test')
prepared_train.save_to_disk("./prepared_datasets/train")
prepared_test.save_to_disk("./prepared_datasets/test")
```

ArtÄ±k ihtiyacÄ±mÄ±z olduÄŸunda kaydettiÄŸimiz bu Ã¶n-iÅŸlenmiÅŸ veri kÃ¼melerini tekrar geri yÃ¼kleyip iÅŸlerimize devam edebiliriz:

```python
prepared_train = load_from_disk("./prepared_datasets/train")
prepared_test = load_from_disk("./prepared_datasets/test")
```

# ViT'i yÃ¼klemek

Elimizdeki gÃ¶rÃ¼ntÃ¼leri kullanacaÄŸÄ±mÄ±z modelin istediÄŸi uygun formata biÃ§imlendirdikten sonra, bir sonraki adÄ±m ViT'yi indirip baÅŸlatmaktÄ±r (initialize).

Burada da, Ã¶znitelik Ã§Ä±karÄ±cÄ±yÄ± (feature extractor) yÃ¼klemek (load) iÃ§in kullandÄ±ÄŸÄ±mÄ±z `from_pretrained` yÃ¶ntemiyle Hugging Face'in `transformers` kÃ¼tÃ¼phanesini kullanÄ±yoruz.

```python
num_classes = prepared_train.features["label"].num_classes
# 8

model = ViTForImageClassification.from_pretrained(model_name,
                                                  num_labels = num_classes,
                                                  label2id   = label2id,
                                                  id2label   = id2label)

#Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']
#You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
```

SÄ±nÄ±flandÄ±rma iÃ§in ViT'ye ince-ayar Ã§ektiÄŸimiz iÃ§in `ViTForImageClassification` sÄ±nÄ±fÄ±nÄ± kullanÄ±yoruz. VarsayÄ±lan olarak bu, yalnÄ±zca iki Ã§Ä±ktÄ±ya sahip bir sÄ±nÄ±flandÄ±rma baÅŸÄ± (classification head) ile modeli baÅŸlatÄ±r.

Elimizdeki Ã¶zel veri kÃ¼mesinde 8 farklÄ± sÄ±nÄ±f var, dolayÄ±sÄ±yla 8 Ã§Ä±ktÄ± ile modeli baÅŸlatmak istediÄŸimizi belirtmek isteriz. Bunu, `num_labels` argÃ¼manÄ±yla gerÃ§ekleÅŸtiririz.

Model mimarisinden anlaÅŸÄ±lacaÄŸÄ± Ã¼zere, ViT model 12 adet kodlayÄ±cÄ±dan (encoder) oluÅŸmaktadÄ±r. Son katman 8 gizli birime (hidden unit, yani nÃ¶ron) sahip sÄ±nÄ±flandÄ±rma katmanÄ±dÄ±r.

```python
model
# ViTForImageClassification(
#   (vit): ViTModel(
#     (embeddings): ViTEmbeddings(
#       (patch_embeddings): ViTPatchEmbeddings(
#         (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
#       )
#       (dropout): Dropout(p=0.0, inplace=False)
#     )
#     (encoder): ViTEncoder(
#       (layer): ModuleList(
#         (0-11): 12 x ViTLayer(
#           (attention): ViTAttention(
#             (attention): ViTSelfAttention(
#               (query): Linear(in_features=768, out_features=768, bias=True)
#               (key): Linear(in_features=768, out_features=768, bias=True)
#               (value): Linear(in_features=768, out_features=768, bias=True)
#               (dropout): Dropout(p=0.0, inplace=False)
#             )
#             (output): ViTSelfOutput(
#               (dense): Linear(in_features=768, out_features=768, bias=True)
#               (dropout): Dropout(p=0.0, inplace=False)
#             )
#           )
#           (intermediate): ViTIntermediate(
#             (dense): Linear(in_features=768, out_features=3072, bias=True)
#             (intermediate_act_fn): GELUActivation()
#           )
#           (output): ViTOutput(
#             (dense): Linear(in_features=3072, out_features=768, bias=True)
#             (dropout): Dropout(p=0.0, inplace=False)
#           )
#           (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
#           (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
#         )
#       )
#     )
#     (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
#   )
#   (classifier): Linear(in_features=768, out_features=8, bias=True)
# )
```

HazÄ±rladÄ±ÄŸÄ±nÄ±z modelin konfigurasyonuna da kolaylÄ±kla eriÅŸebilirsiniz:

```python
model.config
# ViTConfig {
#   "_name_or_path": "google/vit-base-patch16-224-in21k",
#   "architectures": [
#     "ViTModel"
#   ],
#   "attention_probs_dropout_prob": 0.0,
#   "encoder_stride": 16,
#   "hidden_act": "gelu",
#   "hidden_dropout_prob": 0.0,
#   "hidden_size": 768,
#   "id2label": {
#     "0": "dyed-lifted-polyps",
#     "1": "dyed-resection-margins",
#     "2": "esophagitis",
#     "3": "normal-cecum",
#     "4": "normal-pylorus",
#     "5": "normal-z-line",
#     "6": "polyps",
#     "7": "ulcerative-colitis"
#   },
#   "image_size": 224,
#   "initializer_range": 0.02,
#   "intermediate_size": 3072,
#   "label2id": {
#     "dyed-lifted-polyps": "0",
#     "dyed-resection-margins": "1",
#     "esophagitis": "2",
#     "normal-cecum": "3",
#     "normal-pylorus": "4",
#     "normal-z-line": "5",
#     "polyps": "6",
#     "ulcerative-colitis": "7"
#   },
#   "layer_norm_eps": 1e-12,
#   "model_type": "vit",
#   "num_attention_heads": 12,
#   "num_channels": 3,
#   "num_hidden_layers": 12,
#   "patch_size": 16,
#   "qkv_bias": true,
#   "transformers_version": "4.34.0"
# }
```

ArtÄ±k ince-ayar Ã§ekmeye hazÄ±rÄ±z!

# Modele Ä°nce-Ayar Ã‡ekme

HuggingFace'in `Trainer` fonksiyonunu (https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.Trainer) kullanarak ince-ayar Ã§ekeceÄŸiz. `Trainer`, transformer modelleri iÃ§in PyTorch'ta implement edilmiÅŸ, soyutlaÅŸtÄ±rÄ±lmÄ±ÅŸ bir eÄŸitim ve deÄŸerlendirme dÃ¶ngÃ¼sÃ¼dÃ¼r.

Ancak modeli eÄŸitmeye geÃ§meden Ã¶nce gerÃ§ekleÅŸtirmemiz gereken bir kaÃ§ iÅŸlem daha vardÄ±r.

# DeÄŸerlendirme Metriklerini Belirleme

Ä°lk olarak modeli deÄŸerlendirirken kullanacaÄŸÄ±mÄ±z metrikleri (Ã¶lÃ§Ã¼tleri) tanÄ±mlamamÄ±z gerekmektedir. Bu metrikleri Hugging Face'in `evaluate` modÃ¼lÃ¼nden kolaylÄ±kla yÃ¼kleyebilirsiniz - https://huggingface.co/docs/evaluate/index

`evaluate` modÃ¼lÃ¼ 100'den fazla deÄŸerlendirme metriÄŸi iÃ§ermektedir:

```python
import evaluate
# DeÄŸerlendir metriklerinin sayÄ±sÄ±
print(f"Hugging Face'te {len(evaluate.list_evaluation_modules())} adet deÄŸerlendirme metriÄŸi vardÄ±r.\n")
# Hugging Face'te 141 adet deÄŸerlendirme metriÄŸi vardÄ±r.
```

Bu metriklerin ne olduÄŸu ÅŸu ÅŸekilde gÃ¶rÃ¼lebilir:

```python
# TÃ¼m deÄŸerlendirme metriklerini listele
evaluate.list_evaluation_modules()
# ['precision',
#  'code_eval',
#  'roc_auc',
#  'cuad',
#  'xnli',
#  'rouge',
#  'pearsonr'
#   ...
#  'ybelkada/toxicity',
#  'ronaldahmed/ccl_win',
#  'meg/perplexity',
#  'cakiki/tokens_per_byte',
#  'lsy641/distinct']
```

Bu tutorial kapsamÄ±nda DoÄŸruluk (Accuracy), F1 Skoru (F1 Score) Kesinlik (Precision) ve DuyarlÄ±lÄ±k (Recall) metriklerini kullanacaÄŸÄ±z. Bu metrikleri model eÄŸitimi esnasÄ±nda kullanabilmek iÃ§in, Ã¶nce `evaluate` kÃ¼tÃ¼phanesindeki `load` fonksiyonunu kullanarak bu metrikleri yÃ¼klemeli (load) ve daha sonra yine `evaluate` kÃ¼tÃ¼phanesindeki `compute` fonksiyonu ile hesaplamalarÄ± gerÃ§ekleÅŸtirmek Ã¼zere Ã¶zel bir fonksiyon yaratmalÄ±yÄ±z:

```python
accuracy_metric = evaluate.load("accuracy")
f1_metric = evaluate.load("f1")
precision_metric = evaluate.load("precision")
recall_metric = evaluate.load("recall")

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis = -1)

    results = {}
    results.update(accuracy_metric.compute(predictions=preds, references = labels))
    results.update(f1_metric.compute(predictions=preds, references = labels, average="weighted"))
    results.update(precision_metric.compute(predictions=preds, references = labels, average="weighted"))
    results.update(recall_metric.compute(predictions=preds, references = labels, average="weighted"))
    return results
```

Dikkat edilirse, `compute` fonksiyonu tahminleri (predictions) ve etiketleri (labels) beklemektedir.

# EÄŸitim ArgÃ¼manlarÄ±nÄ± belirleme

YapmamÄ±z gereken diÄŸer bir iÅŸlem, EÄŸiticinin (`Trainer`'Ä±n) ihtiyaÃ§ duyduÄŸu  argÃ¼manlarÄ± tanÄ±mladÄ±ÄŸÄ±mÄ±z `TrainingArguments` isimli konfigÃ¼rasyonlarÄ± yazmaktÄ±r. Hugging Face'in `transformers` kÃ¼tÃ¼phanesi eÄŸitim argÃ¼manlarÄ± olarak bir Ã§ok opsiyon sunmaktadÄ±r. Uygun olanlarÄ± alÄ±p, eÄŸitim esnasÄ±nda tercih edeceÄŸiniz deÄŸerleri atayabilirsiniz - https://huggingface.co/docs/transformers/v4.34.0/en/main_classes/trainer#transformers.TrainingArguments

Bu konfigÃ¼rasyonlar, eÄŸitim parametrelerini (training parameters), kaydetme ayarlarÄ±nÄ± (saving settings) ve gÃ¼nlÃ¼ÄŸe kaydetme ayarlarÄ±nÄ± (logging settings) iÃ§erir:

```python
# Modelin kaydedileceÄŸi dizin
model_dir = "./model"

# Modelin log'larÄ±nÄ±n kaydedileceÄŸi dizin
output_data_dir = "./outputs"

# EÄŸitim iÃ§in gerÃ§ekleÅŸtirilecek toplam epoch sayÄ±sÄ±
num_train_epochs = 5

# EÄŸitim iÃ§in GPU/TPU Ã§ekirdeÄŸi/CPU baÅŸÄ±na yÄ±ÄŸÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (batch size)
per_device_train_batch_size = 16

# DeÄŸerlendirme iÃ§in GPU/TPU Ã§ekirdeÄŸi/CPU baÅŸÄ±na yÄ±ÄŸÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (batch size)
per_device_eval_batch_size = 32

# AdamW optimize edici iÃ§in baÅŸlangÄ±Ã§ â€‹â€‹Ã¶ÄŸrenme oranÄ±
learning_rate = 2e-5

# 0'dan Ã¶ÄŸrenme_oranÄ±na (learning_rate) kadar doÄŸrusal bir Ä±sÄ±nma iÃ§in kullanÄ±lan adÄ±m sayÄ±sÄ±
warmup_steps = 500

# AdamW optimize edicideki tÃ¼m yan parametreleri ve LayerNorm aÄŸÄ±rlÄ±klarÄ± hariÃ§ tÃ¼m katmanlara uygulanacak aÄŸÄ±rlÄ±k azalmasÄ± (weight decay)
weight_decay = 0.01

# En iyi modeli seÃ§mek iÃ§in doÄŸruluk oranÄ±nÄ± kullanalÄ±m
main_metric_for_evaluation = "accuracy"

training_args = TrainingArguments(
    output_dir = model_dir,
    num_train_epochs = num_train_epochs,
    per_device_train_batch_size = per_device_train_batch_size,
    per_device_eval_batch_size = per_device_eval_batch_size,
    warmup_steps = warmup_steps,
    weight_decay = weight_decay,
    evaluation_strategy="epoch",
    save_strategy = "epoch",
    logging_strategy = "epoch",
    logging_dir = f"{output_data_dir}/logs",
    learning_rate = float(learning_rate),
    load_best_model_at_end = True,
    remove_unused_columns=False,
    push_to_hub=False,
    metric_for_best_model = main_metric_for_evaluation)
```

Ek, olarak Google Colab Ã¼zerinde GPU ile Ã§alÄ±ÅŸtÄ±ÄŸÄ±mÄ±zdan, aÅŸaÄŸÄ±daki gibi tanÄ±mlama gerÃ§ekleÅŸtirdikten sonra model operasyonlarÄ±nÄ± GPU Ã¼zerine yerleÅŸtirebiliriz:

```python
# device, model eÄŸitiminin GPU veya CPU Ã¼zerinde gerÃ§ekleÅŸip gerÃ§ekleÅŸmeyeceÄŸine karar verecek
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# device(type='cuda')

model.to(device)
```

Daha sonra, eÄŸitim gÃ¶rÃ¼ntÃ¼lerini yÄ±ÄŸÄ±n bÃ¼yÃ¼klÃ¼ÄŸÃ¼ (batch size) kadar yÄ±ÄŸÄ±nlayacak bir `collate` (tÃ¼rkÃ§esi harmanlama'dÄ±r) fonksiyonu yazmanÄ±z gerekmektedir. `collate` fonksiyonu  Ã§ok sayÄ±da veriyle uÄŸraÅŸÄ±rken kullanÄ±ÅŸlÄ±dÄ±r. Modele besleyeceÄŸimiz gÃ¶rÃ¼ntÃ¼lerden oluÅŸan yÄ±ÄŸÄ±nlar (batches), sÃ¶zlÃ¼klerden oluÅŸan listelerdir, dolayÄ±sÄ±yla `collate` yÄ±ÄŸÄ±nlaÅŸtÄ±rÄ±lmÄ±ÅŸ tensÃ¶rler oluÅŸturmamÄ±za yardÄ±mcÄ± olacaktÄ±r.

```python
def collate_fn(batch):
    return {
        'pixel_values': torch.tensor([x['pixel_values'] for x in batch]),
        'labels': torch.tensor([x['label'] for x in batch])
    }
```

ArtÄ±k hazÄ±rÄ±z! Åimdi bir `Trainer` Ã¶rneÄŸi (instance) yaratalÄ±m:

```python
trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    compute_metrics=compute_metrics,
    train_dataset=prepared_train,
    eval_dataset=prepared_test,
    tokenizer=feature_extractor,
)
```

Burada, elimizdeki modeli, oluÅŸturduÄŸumuz eÄŸitim argÃ¼manlarÄ±nÄ±, `collate`  fonksiyonunu, Ã¶n-iÅŸlemeden geÃ§irilmiÅŸ eÄŸitim ve test kÃ¼melerini ve modele ait Ã¶znitelik Ã§Ä±karÄ±cÄ±yÄ± (feature extractor) kullanÄ±rÄ±z. 

...ve modeli eÄŸitmeye hazÄ±rÄ±z:

```python
trainer.train()
```

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/ksavir_vit_training_res.png?raw=true)

Modelin 5 epoch boyunca eÄŸitilmesi neredeyse 1 saat 51 dakika sÃ¼rmÃ¼ÅŸtÃ¼r. EÄŸitim sonunda, ince-ayar Ã§ekilmiÅŸ modelin test kÃ¼mesi Ã¼zerindeki performansÄ±nÄ± yukarÄ±daki ekran gÃ¶rÃ¼ntÃ¼sÃ¼nde gÃ¶rebilirsiniz. OldukÃ§a iyi bir sonuÃ§! :)

Malesef ki, Hugging Face'in `transformers` kÃ¼tÃ¼phanesi eÄŸitim kÃ¼mesi Ã¼zerinde her epoch iÃ§in deÄŸerlendirme metriklerini Ã¶lÃ§memektedir. Bunu gerÃ§ekleÅŸtirmek iÃ§in, Ã¶zel bir Geri Ã‡aÄŸÄ±rma (Callback) fonksiyonu yazabilirsiniz.

Model eÄŸitim sonuÃ§larÄ±nÄ± aÅŸaÄŸÄ±daki ÅŸekilde elde edebilirsiniz:

```python
log_history = pd.DataFrame(trainer.state.log_history)
log_history = log_history.fillna(0)
log_history = log_history.groupby(['epoch']).sum()
log_history
```

![](https://raw.githubusercontent.com/mmuratarat/turkish/864a53b512ce0d4029ccfbc265604ec688ec6010/_posts/images/ksavir_vit_training_logs.png)

EÄŸitim kÃ¼mesi Ã¼zerindeki kaybÄ±, test kÃ¼mesi Ã¼zerindeki kaybÄ± ve test kÃ¼mesi Ã¼zerindeki doÄŸruluk oranÄ±nÄ± her epoch iÃ§in ÅŸu ÅŸekilde kolaylÄ±kla gÃ¶rsel olarak inceleyebilirsiniz:

```python
log_history[["loss", "eval_loss", "eval_accuracy"]].plot(subplots=True)
```

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/kvasir_vit_model_progress.png?raw=true)

Hugging Face eÄŸitim kÃ¼mesine (training dataset) ait deÄŸerlendirme metriklerini dÃ¶ndÃ¼rmediÄŸi iÃ§in, en son elde edilen modeli tÃ¼m eÄŸitim kÃ¼mesi Ã¼zerinde Ã§alÄ±ÅŸtÄ±rarak tanÄ±mladÄ±ÄŸÄ±mÄ±z metriklerin deÄŸerlerini `evaluate` fonksiyonu ile elde edebiliriz:

```python
metrics_training = trainer.evaluate(prepared_train)
metrics_training
# {'eval_loss': 0.09327180683612823,
#  'eval_accuracy': 0.9905882352941177,
#  'eval_f1': 0.990587445420025,
#  'eval_precision': 0.9906179579903401,
#  'eval_recall': 0.9905882352941177,
#  'eval_runtime': 992.092,
#  'eval_samples_per_second': 6.854,
#  'eval_steps_per_second': 0.215,
#  'epoch': 5.0}
```

**EK NOT**

EÄŸitim kÃ¼mesine ait metrikleri elde etmek iÃ§in diÄŸer bir yÃ¶ntem `predict` methodunu kullanmaktÄ±r. BÃ¶ylelikle, model tahminlerini alÄ±r ve bu tahminleri gerÃ§ek etiketlerle karÅŸÄ±laÅŸtÄ±rabiliriz.

```python
# EÄŸitim kÃ¼mesi Ã¼zerinde yapÄ±lan tahminler
y_train_predict = trainer.predict(prepared_train)

# Tahminlere gÃ¶z atalÄ±m
y_train_predict
```

Transfer Ã¶ÄŸrenme gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma modeli iÃ§in tahmin edilen lojitler, `predictions` metodu kullanÄ±larak Ã§Ä±karÄ±labilir:

```python
# Tahmin edilen lojitler
y_train_logits = y_train_predict.predictions

# Ä°lk 5 gÃ¶rÃ¼ntÃ¼ye ait model Ã§Ä±ktÄ±larÄ± (lojitler)
y_train_logits[:5]
```

Tek bir gÃ¶rÃ¼ntÃ¼ iÃ§in elde edilen tahminin sekiz sÃ¼tundan oluÅŸtuÄŸunu gÃ¶rÃ¼yoruz. Ä°lk sÃ¼tun, etiket 0 iÃ§in tahmin edilen lojittir ve ikinci sÃ¼tun, etiket 1 iÃ§in tahmin edilen lojittir ve bu bÃ¶yle devam etmektedir. Lojit deÄŸerlerinin toplamÄ± 1'e eÅŸit deÄŸildir Ã§Ã¼nkÃ¼ bu deÄŸerler normalleÅŸtirilmemiÅŸ olasÄ±lÄ±klardÄ±r (diÄŸer bir deyiÅŸle, model Ã§Ä±ktÄ±sÄ±dÄ±r). Ã‡ok-sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma (multi-class classification) yaptÄ±ÄŸÄ±mÄ±z iÃ§in Softmax fonksiyonu kullanarak bu deÄŸerleri normalleÅŸtirebiliriz:

```python
y_train_probabilities = torch.softmax(y_train_logits, dim = -1)
```

Softmax'Ä± uyguladÄ±ktan sonra, her gÃ¶rÃ¼ntÃ¼ iÃ§in tahmin edilen olasÄ±lÄ±ÄŸÄ±n toplamÄ±nÄ±n 1'e eÅŸit olduÄŸunu gÃ¶rebiliriz:

```python
# Ä°lk 5 gÃ¶rÃ¼ntÃ¼ye ait normalleÅŸtirilmiÅŸ olasÄ±lÄ±klar
y_train_probabilities[:5]
```

Tahmin edilen etiketleri elde etmek iÃ§in, her gÃ¶rÃ¼ntÃ¼ iÃ§in etiketlere karÅŸÄ±lÄ±k gelen maksimum olasÄ±lÄ±k indeksini dÃ¶ndÃ¼rmek Ã¼zere numpy kÃ¼tÃ¼phanesinin `argmax` fonksiyonu kullanÄ±lÄ±r.

```python
# model tarafÄ±ndan eÄŸitim kÃ¼mesi Ã¼zerinde tahmin edilen etiketler
y_train_pred_labels = np.argmax(y_train_probabilities, axis=1)

# Ä°lk 5 gÃ¶rÃ¼ntÃ¼ye ait tahmin edilen etiketler
y_train_pred_labels[:5]
```

GerÃ§ek etiketler `y_train_predict.label_ids` kullanÄ±larak Ã§Ä±karÄ±labilir.

```python
# AsÄ±l Etiketler
y_train_actual_labels = y_train_predict.label_ids

# EÄŸitim kÃ¼mesindeki ilk 5 gÃ¶rÃ¼ntÃ¼ye ait gerÃ§ek etiketler
y_train_actual_labels[:5]
```

ArtÄ±k gerÃ§ek etiketleri (actual labels), eÄŸitim kÃ¼mesi Ã¼zerinde model tarafÄ±ndan tahmin edilen etiketler ile karÅŸÄ±laÅŸtÄ±rabiliriz.

Daha fazla model performans metriÄŸi hesaplamak iÃ§in ilgilenilen metrikleri yÃ¼klemek amacÄ±yla `evaluate.load`'u kullanabiliriz. BazÄ± metrikleri zaten yukarÄ±daki hÃ¼crelerin birinde yÃ¼klemiÅŸtik:

```python
# Compute accuracy metric
print(accuracy_metric.compute(predictions=y_train_pred_labels, references=y_train_actual_labels))

# Compute f1 metric
print(f1_metric.compute(predictions=y_train_pred_labels, references=y_train_actual_labels, average="weighted"))

# Compute precision metric
print(precision_metric.compute(predictions=y_train_pred_labels, references=y_train_actual_labels, average="weighted"))

# Compute recall metric
print(recall_metric.compute(predictions=y_train_pred_labels, references=y_train_actual_labels, average="weighted"))
```

# En Ä°yi Modeli Kaydetme

ArtÄ±k sonuÃ§larÄ±mÄ±zdan memnun olduÄŸumuza gÃ¶re en iyi modeli kaydedebiliriz.

```
trainer.save_model(model_dir)
```

YukarÄ±daki kod satÄ±rÄ± hem modeli hem de modelle kullanÄ±lan Ã¶znitelik Ã§Ä±karÄ±cÄ±yÄ± (feature extractor) model dizinine kayÄ±t edecektir.

Ancak, sadece Ã¶znitelik Ã§Ä±karÄ±cÄ±yÄ± kaydetmek isterseniz `feature_extractor.save_pretrained(model_dir)` kodunu kullanabilirsiniz. Bu kod sadece `preprocessor_config.json` dosyasÄ±nÄ± `model_dir` isimli dizine kaydedecektir.

Sonunda ince ayar Ã§ekilmiÅŸ ViT modeline sahibiz! ğŸ¥³ğŸ¥³ğŸ¥³ ğŸ‰ğŸ‰ğŸ‰

# Modeli Test KÃ¼mesi Ãœzerinde DeÄŸerlendirme

YukarÄ±daki adÄ±mlara benzer ÅŸekilde ÅŸimdi test kÃ¼mesindeki performansÄ±nÄ± doÄŸrulamamÄ±z ve deÄŸerlendirme sonuÃ§larÄ±nÄ± (evaluation results) kaydetmemiz gerekmektedir.

```python
metrics = trainer.evaluate(prepared_test)
trainer.log_metrics("eval", metrics)
trainer.save_metrics("eval", metrics)
```

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/ksavir_vit_test_evaluation_res.png?raw=true)

`save_metrics` fonksiyonu test kÃ¼mesi Ã¼zerinde deÄŸerlendirilen modelin sonuÃ§larÄ±nÄ± `model_dir` dizininin (bizim Ã¶rneÄŸimizde `./model` klasÃ¶rÃ¼) altÄ±nda `eval_results.json` olarak kaydedecektir.

Elde ettiÄŸimiz modelin doÄŸruluÄŸu oldukÃ§a iyi.

Ä°htiyaÃ§ halinde test kÃ¼mesinin ince-ayar Ã§ekilmiÅŸ model tarafÄ±ndan tahmin edilen etiketleri de elde edilebilir:

```python
y_test_predict = trainer.predict(prepared_test)
y_test_predict
# PredictionOutput(predictions=array([[-0.5574911 , -0.55256057, -0.44084704, ..., -0.6514604 ,
#         -0.587717  ,  4.1993985 ],
#        [ 4.004751  , -0.9422177 , -0.65568745, ..., -0.59566027,
#         -0.17343669, -0.4465061 ],
#        [-0.87891173, -1.0014176 ,  2.749767  , ...,  2.7222385 ,
#         -1.0784245 , -0.91497976],
#        ...,
#        [-0.6408027 , -0.6666713 , -0.70249856, ..., -0.46141627,
#         -0.4458719 , -0.84069437],
#        [-0.6629647 , -0.72915053, -0.6185216 , ..., -0.6192481 ,
#         -0.17926458, -0.71570873],
#        [-0.6530872 , -0.67923456, -0.50120807, ..., -0.61496264,
#         -0.73776233, -0.31934953]], dtype=float32), label_ids=array([7, 0, 5, ..., 4, 3, 3]), metrics={'test_loss': 0.23830144107341766, 'test_accuracy': 0.9391666666666667, 'test_f1': 0.9390956371766551, 'test_precision': 0.939918876802155, 'test_recall': 0.9391666666666667, 'test_runtime': 313.2723, 'test_samples_per_second': 3.831, 'test_steps_per_second': 0.121})
```

Transfer Ã¶ÄŸrenme gÃ¶rÃ¼ntÃ¼ sÄ±nÄ±flandÄ±rma modeli iÃ§in tahmin edilen lojitler, `predictions` metodu kullanÄ±larak Ã§Ä±karÄ±labilir:

```python
# Tahmin edilen lojitler
y_test_logits = y_test_predict.predictions

# Ä°lk 5 gÃ¶rÃ¼ntÃ¼ye ait model Ã§Ä±ktÄ±larÄ± (lojitler)
y_test_logits[:5]
# array([[-0.5574911 , -0.55256057, -0.44084704, -0.43077588, -0.69217765,
#         -0.6514604 , -0.587717  ,  4.1993985 ],
#        [ 4.004751  , -0.9422177 , -0.65568745, -0.64027154, -0.7201288 ,
#         -0.59566027, -0.17343669, -0.4465061 ],
#        [-0.87891173, -1.0014176 ,  2.749767  , -0.8135734 , -1.0272567 ,
#          2.7222385 , -1.0784245 , -0.91497976],
#        [-0.44040072, -0.60538346, -0.6533643 , -0.5478527 , -0.48782963,
#         -0.50029   ,  4.1607065 , -0.6397783 ],
#        [-0.66559803, -0.6086572 , -0.5033142 , -0.36208436, -0.53697765,
#         -0.7260709 , -0.3696426 ,  4.2133403 ]], dtype=float32)
```

Tek bir gÃ¶rÃ¼ntÃ¼ iÃ§in elde edilen tahminin sekiz sÃ¼tundan oluÅŸtuÄŸunu gÃ¶rÃ¼yoruz. Ä°lk sÃ¼tun, etiket 0 iÃ§in tahmin edilen lojittir ve ikinci sÃ¼tun, etiket 1 iÃ§in tahmin edilen lojittir ve bu bÃ¶yle devam etmektedir. Lojit deÄŸerlerinin toplamÄ± 1'e eÅŸit deÄŸildir Ã§Ã¼nkÃ¼ bu deÄŸerler normalleÅŸtirilmemiÅŸ olasÄ±lÄ±klardÄ±r (diÄŸer bir deyiÅŸle, model Ã§Ä±ktÄ±sÄ±dÄ±r). Ã‡ok-sÄ±nÄ±flÄ± sÄ±nÄ±flandÄ±rma (multi-class classification) yaptÄ±ÄŸÄ±mÄ±z iÃ§in Softmax fonksiyonunu kullanarak bu deÄŸerleri normalleÅŸtirebiliriz:

```python
y_test_probabilities = torch.softmax(torch.tensor(y_test_logits), dim = 1)
```

Softmax'Ä± uyguladÄ±ktan sonra, her gÃ¶rÃ¼ntÃ¼ iÃ§in tahmin edilen olasÄ±lÄ±klarÄ±n toplamÄ±nÄ±n 1'e eÅŸit olduÄŸunu gÃ¶rebiliriz:

```python
# Ä°lk 5 gÃ¶rÃ¼ntÃ¼ye ait normalleÅŸtirilmiÅŸ olasÄ±lÄ±klar
y_test_probabilities[:5]
# tensor([[0.0081, 0.0081, 0.0091, 0.0092, 0.0071, 0.0074, 0.0079, 0.9431],
#         [0.9328, 0.0066, 0.0088, 0.0090, 0.0083, 0.0094, 0.0143, 0.0109],
#         [0.0125, 0.0111, 0.4714, 0.0134, 0.0108, 0.4586, 0.0103, 0.0121],
#         [0.0094, 0.0080, 0.0076, 0.0085, 0.0090, 0.0089, 0.9408, 0.0077],
#         [0.0072, 0.0076, 0.0084, 0.0097, 0.0082, 0.0067, 0.0096, 0.9426]])
```

Tahmin edilen etiketleri elde etmek iÃ§in, her gÃ¶rÃ¼ntÃ¼ iÃ§in etiketlere karÅŸÄ±lÄ±k gelen maksimum olasÄ±lÄ±ÄŸa sahip indeksi dÃ¶ndÃ¼rmek Ã¼zere NumPy kÃ¼tÃ¼phanesinin `argmax` fonksiyonu kullanÄ±lÄ±r:

```python
# model tarafÄ±ndan eÄŸitim kÃ¼mesi Ã¼zerinde tahmin edilen etiketler
y_test_pred_labels = np.argmax(y_test_probabilities, axis=1)

# Ä°lk 5 gÃ¶rÃ¼ntÃ¼ye ait tahmin edilen etiketler
y_test_pred_labels[:5]
# tensor([7, 0, 2, 6, 7])
```

GerÃ§ek etiketler `y_test_predict.label_ids` kullanÄ±larak Ã§Ä±karÄ±labilir.

```python
# AsÄ±l Etiketler
y_test_actual_labels = y_test_predict.label_ids

# EÄŸitim kÃ¼mesindeki ilk 5 gÃ¶rÃ¼ntÃ¼ye ait gerÃ§ek etiketler
y_test_actual_labels[:5]
# array([7, 0, 5, 6, 7])
```

ArtÄ±k gerÃ§ek etiketleri (actual labels), eÄŸitim kÃ¼mesi Ã¼zerinde model tarafÄ±ndan tahmin edilen etiketler ile karÅŸÄ±laÅŸtÄ±rabiliriz.

Daha fazla model performans metriÄŸi hesaplamak iÃ§in ilgilenilen metrikleri yÃ¼klemek amacÄ±yla `evaluate.load`'u kullanabiliriz. BazÄ± metrikleri zaten yukarÄ±daki hÃ¼crelerin birinde yÃ¼klemiÅŸtik:

```python
# Compute accuracy metric
print(accuracy_metric.compute(predictions=y_test_pred_labels, references=y_test_actual_labels))

# Compute f1 metric
print(f1_metric.compute(predictions=y_test_pred_labels, references=y_test_actual_labels, average="weighted"))

# Compute precision metric
print(precision_metric.compute(predictions=y_test_pred_labels, references=y_test_actual_labels, average="weighted"))

# Compute recall metric
print(recall_metric.compute(predictions=y_test_pred_labels, references=y_test_actual_labels, average="weighted"))

# {'accuracy': 0.9391666666666667}
# {'f1': 0.9390956371766551}
# {'precision': 0.939918876802155}
# {'recall': 0.9391666666666667}
```

KolaylÄ±kla anlaÅŸÄ±lacaÄŸÄ± Ã¼zere elde edilen sonuÃ§lar, `trainer.evaluate(prepared_test)` kod satÄ±rÄ±nÄ±n dÃ¶ndÃ¼rdÃ¼ÄŸÃ¼ sonuÃ§lar ile aynÄ±dÄ±r!

# Tek Bir GÃ¶rÃ¼ntÃ¼ Ãœzerinde Modelin Tahmini

Åimdi de rastgele bir gÃ¶rÃ¼ntÃ¼nÃ¼n sÄ±nÄ±f tahmini (class prediction) elde edelim. Test veri kÃ¼memizdeki bir gÃ¶rseli seÃ§ip tahmin edilen etiketin doÄŸru olup olmadÄ±ÄŸÄ±nÄ± gÃ¶rebiliriz.

```python
image = test_dataset["image"][0]
image
```

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/ksavirTestOneImage.png?raw=true)

Bu gÃ¶rÃ¼ntÃ¼ye ait asÄ±l etiketi (actual label) bulalÄ±m:

```python
# extract the actual label of the first image of the testing dataset
actual_label = id2label[str(test_dataset["label"][0])]
actual_label
# 'ulcerative-colitis'
```

GÃ¶rÃ¼ntÃ¼nÃ¼n bir `ulcerative-colitis` sÄ±nÄ±fÄ±na ait olduÄŸunu gÃ¶rÃ¼yoruz. Åimdi modelimizin ne tahmin ettiÄŸini gÃ¶relim.

Bunun iÃ§in modelimizi bir daha yÃ¼klÃ¼yoruz:

```python
model_finetuned = ViTForImageClassification.from_pretrained(model_dir)
feature_extractor_finetuned = ViTFeatureExtractor.from_pretrained(model_dir)
```

Daha sonra orijinal test gÃ¶rÃ¼ntÃ¼sÃ¼nÃ¼ eÄŸittiÄŸimiz modele ait Ã¶znitelik Ã§Ä±karÄ±cÄ±dan (feature extractor) geÃ§iriyoruz ve elde edilen tensÃ¶rÃ¼ modelimize besliyoruz.

Burada `no_grad`, yalnÄ±zca Ã§Ä±karsama (inference) yaptÄ±ÄŸÄ±mÄ±z iÃ§in gradyan hesaplamasÄ±nÄ± devre dÄ±ÅŸÄ± bÄ±rakan bir baÄŸlam yÃ¶neticisidir (context manager).

```python
inputs = feature_extractor_finetuned(image, return_tensors="pt")

with torch.no_grad():
    logits = model_finetuned(**inputs).logits

logits
# tensor([[-0.5575, -0.5526, -0.4408, -0.4308, -0.6922, -0.6515, -0.5877,  4.1994]])
```

Elde edilen tensor her 8 sÄ±nÄ±fa ait lojit deÄŸerleridir.

Lojitler Ã¼zerinde NumPy kÃ¼tÃ¼phanesinin `argmax` fonksiyonunu Ã§aÄŸÄ±rdÄ±ÄŸÄ±mÄ±zda, en yÃ¼ksek olasÄ±lÄ±ÄŸa sahip sÄ±nÄ±fÄ±n indeksini alÄ±rsÄ±nÄ±z:

```python
predicted_label = logits.argmax(-1).item()
predicted_label
# 7

predicted_class = id2label[str(predicted_label)]
predicted_class
# 'ulcerative-colitis'
```

Ä°nce ayar Ã§ekilmiÅŸ modelin tahmini de `ulcerative-colitis`! Tam da beklediÄŸimiz gibi!

# Ä°nce Ayar Ã‡ekilmiÅŸ Modeli Hugging Face Hub'a Push'lamak

Modelimizin deÄŸerlendirme aÅŸamasÄ±nÄ± da tamamladÄ±ktan sonra baÅŸkalarÄ± tarafÄ±ndan kullanÄ±lmak Ã¼zere Hugging Face'in Hub'Ä±na push'layabiliriz!

```python
notebook_login()
```

` notebook_login()` fonksiyonunu Ã§alÄ±ÅŸtÄ±rdÄ±ktan sonra sizden bir token oluÅŸturmanÄ±zÄ± ve bu oluÅŸturduÄŸunuz token'Ä± ekrana girmenizi isteyecektir:

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/huggingface_notebook_login.png?raw=true)

Token oluÅŸturmak iÃ§in https://huggingface.co/settings/tokens sayfasÄ±na gidiniz, `New Token` butonuna tÄ±klayÄ±nÄ±z. Token ismini (`Name`) giriniz. `Role` olarak da `write` seÃ§meyi unutmayÄ±nÄ±z. Ã‡Ã¼nkÃ¼ daha sonra Hugging Face Hub'da bir repo oluÅŸturacaÄŸÄ±z ve bu repoya dosyalarÄ± ekleyebilmek iÃ§in yazma izinlerine (write permissions) sahip olmamÄ±z gerekmektedir:

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/image_huggingface_token_creation.png?raw=true)

Token'Ä± ilgili boÅŸluÄŸa girip `Login` butonuna bastÄ±ÄŸÄ±nÄ±z zaman `Login successful` Ã§Ä±ktÄ±sÄ±nÄ± aldÄ±ÄŸÄ±nÄ±zdan emin olunuz:

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/huggingface_login_success.png?raw=true)

Notebook Ã¼zerinden Hugging Face ortamÄ±na giriÅŸ yaptÄ±ktan sonra, ilk olarak Hub'da bir depo (respository) oluÅŸturmamÄ±z gerekmektedir. `create_repo` fonksiyonuna kullanÄ±cÄ± adÄ±nÄ±zÄ± ve oluÅŸturacaÄŸÄ±nÄ±z deponun ismini girmeniz istenir:

```python
create_repo("mmuratarat/kvasir-v2-classifier", private=False)
# RepoUrl('https://huggingface.co/mmuratarat/kvasir-v2-classifier', endpoint='https://huggingface.co', repo_type='model', repo_id='mmuratarat/kvasir-v2-classifier')
```

Depoyu oluÅŸturduktan sonra artÄ±k Kvasir veri kÃ¼mesi iÃ§in ince ayar Ã§ektiÄŸimiz Visual Transformer modelini Hub'a gÃ¶nderebiliriz (push'layabiliriz):

```python
model_finetuned.push_to_hub("mmuratarat/kvasir-v2-classifier")
feature_extractor_finetuned.push_to_hub("mmuratarat/kvasir-v2-classifier")
```

ArtÄ±k modelimizi herkes kolaylÄ±kla kullanabilir.

Modelin sayfasÄ±na https://huggingface.co/mmuratarat/kvasir-v2-classifier baÄŸlantÄ±sÄ±ndan ulaÅŸabilirsiniz.

**NOT**: Bu yÃ¶ntem otomatik olarak bir model kartÄ± yaratmaz. Bu nedenle, baÅŸkalarÄ±nÄ±n sizin Ã§alÄ±ÅŸmanÄ±zÄ± kolay anlamasÄ± iÃ§in bir model kartÄ± yaratmayÄ± unutmayÄ±nÄ±z!

# Hugging Face'in Auto SÄ±nÄ±flarÄ±nÄ± Kullanarak Hub'daki Ä°nce Ayar Ã‡ekilmiÅŸ Modele EriÅŸmek

ArtÄ±k Hugging Face'in Auto sÄ±nÄ±flarÄ±nÄ± kullanabiliriz - https://huggingface.co/docs/transformers/v4.33.3/en/model_doc/auto#auto-classes

Ã‡oÄŸu durumda kullanmak istediÄŸiniz mimari, `from_pretrained()` metoduna saÄŸladÄ±ÄŸÄ±nÄ±z Ã¶nceden eÄŸitilmiÅŸ modelin adÄ±ndan (name) veya yolundan (path) tahmin edilebilir.

Auto sÄ±nflar bu iÅŸi sizin iÃ§in yapmak iÃ§in buradalar; bÃ¶ylelikle, adÄ±/yolu verilen Ã¶nceden-eÄŸitilmiÅŸ (pre-trained) ilgili modeli ve bu modelin Ã¶nceden-eÄŸitilmiÅŸ aÄŸÄ±rlÄ±klarÄ±nÄ± (weights), konfigÃ¼rasyonlarÄ±nÄ± (config) ve kelime hazinesini (vocabularÄ±) otomatik olarak alÄ±rsÄ±nÄ±z.

`AutoModelForImageClassification` sÄ±nÄ±fÄ± ile elde ettiÄŸimiz modeli Hub'dan istediÄŸimiz zaman Ã§ekerek Ã§Ä±karsamalar yapabilirsiniz. Ancak girdi gÃ¶rÃ¼ntÃ¼lerini Ã¶nce bir Ã¶znitelik Ã§Ä±karÄ±cÄ±dan geÃ§irmemiz gerekmektedir. Bunu ise `AutoFeatureExtractor` sÄ±nÄ±fÄ± ile gerÃ§ekleÅŸtirebilirsiniz. SonuÃ§ta eÄŸittiÄŸimiz modelde kullanÄ±lan Ã¶znitelik Ã§Ä±karÄ±cÄ± (feature extractor) model ile birlikte `preprocessor_config.json` isimli bir JSON dosyasÄ±na kaydedildi ve Hub'a gÃ¶nderildi.

Buraya not dÃ¼ÅŸÃ¼lmesi gereken bir baÅŸka konu ise, modeli eÄŸitirken kullanÄ±lan argÃ¼manlarÄ±n da bir JSON dosyasÄ± olarak kaydedildiÄŸidir. Bu dosya bir konfigurasyon dosyasÄ±dÄ±r ve `config.json` ismiyle bir JSON dosyasÄ± ÅŸeklinde model dizinin altÄ±na kaydedilmiÅŸ ve Hub'a gÃ¶nderilmiÅŸtir.

```python
model = AutoModelForImageClassification.from_pretrained("mmuratarat/kvasir-v2-classifier")
feature_extractor = AutoFeatureExtractor.from_pretrained("mmuratarat/kvasir-v2-classifier")
```

Test kÃ¼mesindeki 582. gÃ¶rÃ¼ntÃ¼yÃ¼ alalÄ±m:

```python
image = test_dataset["image"][582]
actual_label = id2label[str(test_dataset["label"][582])]
actual_label
# 'normal-z-line'
```

Bu gÃ¶rÃ¼ntÃ¼nÃ¼n gerÃ§ek sÄ±nÄ±fÄ± `normal-z-line`'dÄ±r.

Åimdi indirdiÄŸimiz modelin tahminin elde edelim:

```python
inputs = feature_extractor(image, return_tensors="pt")

with torch.no_grad():
    logits = model(**inputs).logits

predicted_label = logits.argmax(-1).item()
predicted_class = id2label[str(predicted_label)]
predicted_class
# 'normal-z-line'
```

DoÄŸru cevap!

# Inference API'sÄ±nÄ± Kullanarak GÃ¶rÃ¼ntÃ¼nÃ¼n SÄ±nÄ±f Tahmini

BilgisayarÄ±nÄ±zda yerel olarak bir gÃ¶rÃ¼ntÃ¼nÃ¼n sÄ±nÄ±fÄ±nÄ± nasÄ±l tahmin edebileceÄŸinizi yukarÄ±da gÃ¶rdÃ¼k. Buna ek olarak, Hugging Face'in saÄŸladÄ±ÄŸÄ± bir kolaylÄ±k olan Inference API'sÄ±nÄ± da kullanabilirsiniz. TarayÄ±cÄ±nÄ±z (browser) Ã¼zerinden gastrointestinal sistemin iÃ§erisinden alÄ±nan istediÄŸiniz bir gÃ¶rÃ¼ntÃ¼nÃ¼n tahmini elde edebilirsiniz:

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/ksavir_vit_hostedinferenceAP_1.png?raw=true)

Tek yapmanÄ±z gereken bilgisayarÄ±nÄ±zdan gÃ¶rÃ¼ntÃ¼yÃ¼ upload etmek. Åimdi `polyps` sÄ±nÄ±fÄ±ndan bir gÃ¶rÃ¼ntÃ¼yÃ¼ (gÃ¶rÃ¼ntÃ¼yÃ¼ [buradan](https://github.com/mmuratarat/turkish/blob/master/_posts/images/example_polyps_image.jpg?raw=true) indirebilirsiniz) buraya gÃ¶nderelim:

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/ksavir_vit_hostedinferenceAP_2.png?raw=true)

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/ksavir_vit_hostedinferenceAP_3.png?raw=true)

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/ksavir_vit_hostedinferenceAP_4.png?raw=true)

KolaylÄ±kla anlaÅŸÄ±lacaÄŸÄ± Ã¼zere, model, %94.1 olasÄ±lÄ±kla bu gÃ¶rÃ¼ntÃ¼nÃ¼n `polyps` sÄ±nÄ±fÄ±na ait olduÄŸunu doÄŸru bir ÅŸekilde tahmin etmiÅŸtir!
