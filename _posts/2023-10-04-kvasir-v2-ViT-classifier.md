---
layout: post
title: "nceden Eitilmi Visual Transformer (ViT) modeline 襤nce-Ayar ekmek"
author: "MMA"
comments: true
---

S覺f覺rdan eitim (training from scratch), bir modelin tamamen yeni bir g繹rev batan sona eitilmesini i癟erir. Bu genellikle b羹y羹k veri setleri ve y羹ksek hesaplama g羹c羹 (computation power) gerektirir. Ayr覺ca, eitim s羹reci genellikle g羹nler veya haftalar s羹rebilir.Bu y繹ntem genellikle 繹zel bir g繹rev veya dil modeli oluturmak isteyen arat覺rmac覺lar ve b羹y羹k irketler taraf覺ndan kullan覺l覺r.

Ancak, bu ii hobi olarak yapan biri veya bir 繹renci i癟in bir modeli s覺f覺rdan oluturmak o kadar kolay deildir. B羹y羹k veri ve y羹ksek hesaplama g羹c羹n羹n yan覺nda, ayn覺 zamanda oluturulacak modelin mimarisini (architecture) belirleme ve bu modele ait hiperparametreleri ayarlama (hyperparameter tuning) s羹reci de zorludur. 

Bu sebeple, transfer 繹renme (transfer learning) ad覺 verilen bir konsept literat羹rde yerini alm覺t覺r. 

renimleri bir problemden yeni ve farkl覺 bir probleme uyarlamak Transfer renme fikrini temsil eder. 繹yle d羹羹n羹rsek insan覺n 繹renmesi b羹y羹k 繹l癟羹de bu 繹renme yakla覺m覺na dayanmaktad覺r. Transfer 繹renimi sayesinde Java 繹renmek size olduk癟a kolay gelebilir 癟羹nk羹 繹renme s羹recine girildiinde zaten programlama kavramlar覺n覺 ve Python s繹zdizimini anl覺yorsunuzdur.

Ayn覺 mant覺k derin 繹renme (deep learning) i癟in de ge癟erlidir. Transfer 繹renme, genellikle 繹nceden-eitilmi (pre-trained) bir modelin (繹rnein, Hugging Face taraf覺ndan salanan bir dil modeli) 繹zel bir g繹rev veya veri k羹mesine uyarlanmas覺d覺r. Dier bir deyile, 繹nceden eitilmi bir modelin a覺rl覺klar覺 yeni veriler 羹zerinde eitilir. B繹ylelikle, 繹nceden eitilmi model yeni bir g繹rev i癟in haz覺r hale gelir. 

nceden eitilmi bir model kullanman覺n 繹nemli faydalar覺 vard覺r. Hesaplama maliyetlerini ve karbon ayak izinizi azalt覺r ve s覺f覺rdan eitim alman覺za gerek kalmadan son teknoloji 羹r羹n羹 modelleri kullanman覺za olanak tan覺r

 Hugging Face Transformers, 癟ok 癟eitli g繹revler i癟in (繹rnein, doal dil ileme ve bilgisayarl覺 g繹r羹) 繹nceden eitilmi binlerce modele eriim salar (https://huggingface.co/models). nceden eitilmi bir model kulland覺覺n覺zda, onu g繹revinize 繹zel bir veri k羹mesi 羹zerinde eitirsiniz. Bu, inan覺lmaz derecede g羹癟l羹 bir eitim teknii olan ince-ayar (fine-tuning) olarak bilinir.

Transformer-tabanl覺 modeller genellikle g繹revden ba覺ms覺z g繹vde (task-independent body) ve g繹reve 繹zel kafa (task-specific head) olarak ikiye ayr覺l覺r. Genellikle g繹revden ba覺ms覺z k覺s覺m, Hugging Face taraf覺ndan salanan a覺rl覺klara (weights) sahiptir. Bu k覺s覺mdaki a覺rl覺klar dondurulmutur ve herhangi bir g羹ncellemeye (updates) sahip olmazlar. G繹reve 繹zel kafa'da, elinizdeki g繹rev i癟in ihtiyac覺n覺z kadar n繹ron oluturulur ve sadece bu katmanda eitim 繹zel veri k羹meniz kullan覺larak ger癟ekletirilir.

![](https://github.com/mmuratarat/turkish/blob/master/_posts/images/fine_tuning_example.png?raw=true)

Ancak, ince ayar, sinir a覺n覺n tamam覺nda veya yaln覺zca katmanlar覺n覺n bir alt k羹mesinde yap覺labilir; bu durumda, ince ayar覺 yap覺lmayan katmanlar "dondurulur (frozen)" (geri yay覺l覺m (backpropagation) ad覺m覺 s覺ras覺nda g羹ncellenmez).

襤te, bu tutorial'da 繹zel bir veri k羹mesi (a custom dataset) i癟in 繹nceden eitilmi bir modele ince ayar yapacaks覺n覺z.
