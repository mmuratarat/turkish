---
layout: post
title:  "Kayıp Verileri Ele Alma - 1. Bölüm"
author: "MMA"
comments: true
---

Bazı durumlarda veriler, önceden tanımlanmış özniteliklere (değişkenlere) sahip bir veri kümesi biçiminde gelir. Bazı durumlarda, bazı özniteliklerin değerleri eksik olabilir. Bu genellikle, veri kümesi el yapımı olduğunda ve bu veri kümesi üzerinde çalışan kişi bazı değerleri doldurmayı unuttuğunda veya bunları hiç ölçmediğinde gerçekleşir.

Kayıp gözlemler ile ilgili kullanılabilecek yaklaşımlar, problemin türüne bağlıdır. Bu nedenle, kayıp gözlemlerle başa çıkmanın iyi bir yolu olmadığını güvenle söyleyebiliriz. Birkaç teknik denemek, birkaç model oluşturmak ve işe yarayan birini seçmek en iyisi olabilir.

Model oluşturmayı tamamlayıp, tahmin elde etmek sürecine başladığınızda, gözleminiz tam (complete) değilse, var olan kayıp gözlemi veya gözlemleri doldurmak için eğitim verilerini tamamlamak için kullandığınız teknikle aynı veri impütasyon tekniğini kullanmalısınız.

Özetlemek gerekirse, veri imputasyonu (data imputation) zordur ve dikkatle yapılmalıdır. İmputasyon için hangi algoritmanın kullanılacağına karar verirken kayıp olan verilerin doğasını anlamak önemlidir.

Kaynağına bağlı olarak, kayıp verileri tanımlamak için aşağıdaki terminoloji kullanılır:

* **Rastgele Kayıp (Missing at Random (MaR))**: Bu kayıp veri kategorisi, anketin tasarlanma şekli nedeniyle cevaplanamayan niteliklere atıfta bulunur. Örneğin, bir ankette aşağıdaki soruyu düşünün:
  (a) Sigara içiyor musunuz? Evet / Hayır (b) Evet ise, ne sıklıkla? Haftada bir, günde bir, günde iki defa, günde 2 defadan fazla.
  
  (b) sorusunun cevabının ancak (a) sorusunun cevabı "Evet" ise verilebileceğini görebilirsiniz. Veri setindeki bu tür kayıp değerler, bir özniteliğin başka bir özniteliğe bağımlı olmasından kaynaklanmaktadır.
  
* **Tamamen Rastgele Kayıp (Missing Completely at Random (MCaR))**: Bu kayıp veri kategorisi, gerçekten gözden kaçan veriler veya dikkatsizlik veya başka nedenlerle yakalanamayan verilerdir. Bu, kaybedilen verinin incelenen kişi ile ilgisi olmadığı anlamına gelir. Örneğin, bir anket kaybolabilir veya laboratuvarda bir kan örneği hasar görebilir.

* **Rastgele Olmayan Kayıp (Missing Not at Random (MNaR))**: Bu kayıp veri kategorisi, verinin kendisinin alacağı değere bağlıdır. Örneğin, bir anket ile insanların 10. sınıfta Kimya dersinden aldıkları notları elde etmeye çalıştığımızı düşünelim. Düşük not almış olan kişiler notlarını açıklamamayı tercih edebilir, bu nedenle veri kümesinde sadece yüksek notlar görürsünüz.

## Kayıp gözlemleri ele almak için kullanılabilecek yöntemler

Birçok makine öğrenmesi algoritması kayıp gözlemleri desteklemediğinden, veri kümesinin ön işlemesi (pre-processing) sırasında kayıp gözlemlerin ele alınması çok önemlidir. Kayıp gözlemlerin ele alınması, analistlerin karşılaştığı en büyük zorluklardan biridir, çünkü bunların nasıl ele alınacağına dair doğru kararı vermek, sağlam veri modelleri oluşturur. Eksik değerleri atamanın farklı yollarına bakalım.

1. **Veri satırını veya tüm sütunu yoksay**: Veri kümeniz yeterince büyükse, bazı eğitim örneklerini gözden çıkartabilirsiniz. Bu hızlı bir çözümdür ve tipik olarak kayıp gözlemlerin yüzdesinin nispeten düşük olduğu ($< \%5$) durumlarda tercih edilir. Kayıp gözlemlerden kurtulurken kullanabileceğiniz kirli bir yaklaşımdır ancak bilgi kaybına neden olabilir. Bununla birlikte, bazı durumlarda değişkenlerin geri kalanı mükemmel bir şekilde doldurulmuş ve bilgilendirici olsa bile, özniteliklerden (değişkenlerden) biri kayıp bir değere sahip olduğu için bir bütün gözlemi (satırı) veriden atmak, yapılmaması gereken bir durumdur. Bu nedenle, yalnızca satırdaki tüm değerler kayıp ise satırları bırakmayı seçmelisiniz. Bazen yalnızca en az 4 `NA`-olmayan değere sahip satırları tutmak isteyebilirsiniz (4 burada rastgele bir sayıdır). Bazen bazı kayıp değerleri olan bir sütundan (değişkenden) kurtulmak isteyebilirsiniz. Ancak bu yaklaşım, yalnızca verilerde $\%60$'tan fazla gözlemin kayıp olması ve bu değişkenin önemsiz olması durumunda geçerlidir.

2. **Kayıp gözlemi, sabit değer aralığının dışında bir sabitle değiştirin, $-999$, $-1$ vb.**: Başka bir teknik, kayıp değeri, bir özniteliğin alabileceği değerlerin aralığının dışındaki bir değerle değiştirmektir. Örneğin, normal aralık $[0,1]$ ise eksik değeri $2$ veya $-1$ olarak ayarlayabilirsiniz. Buradaki fikir, öğrenme algoritmasının, öznitelik normal değerlerden önemli ölçüde farklı bir değere sahip olduğunda yapılacak en iyi şeyin ne olduğunu öğreneceğidir. Alternatif olarak, kayıp değeri aralığın ortasındaki bir değerle değiştirebilirsiniz. Örneğin, bir öznitelğin aralığı $[−1,1]$ ise, kayıp değeri $0$'a eşit olarak ayarlayabilirsiniz. Buradaki fikir, aralığın ortasındaki değerin tahminleri önemli ölçüde etkilemeyeceğidir.

3. **Ortalama ve medyan değerle değiştirin**: Bu basit imputasyon yöntemi, her değişkeni (sütun) ayrı ayrı ele almaya, diğer değişkenlerle herhangi bir karşılıklı ilişkiyi göz ardı etmeye dayanır. Ortalama, aykırı değerler içermeyen sürekli veriler için uygundur. Medyan, aykırı değerlere sahip sürekli veriler için uygundur. Kategorik öznitelikler için, kayıp değerleri en yaygın değer (the most frequent - mod) ile doldurmayı seçebilirsiniz. Ortalama, medyan ve mod atamasının, impute edilen değişken(ler) arasındaki herhangi bir korelasyonu azalttığına dikkat ediniz. Bunun nedeni, impute edilen değişken ile diğer ölçülen değişkenler arasında bir ilişki olmadığını varsaymamızdır. Bu nedenle, bu imputasyon yöntemi, tek değişkenli (univariate) analiz için bazı çekici özelliklere sahiptir, ancak çok değişkenli (multivariate) analiz için sorunlu hale gelir.

4. **Hiçbir şey yapmayın**: Bu kolay bir çözümdür. Algoritmanın kayıp verileri ele almasına izin verin. Bazı algoritmalar (örneğin, XGBoost), kayıp değerleri hesaba katabilir ve eğitim kaybının azaltılmasına dayalı olarak kayıp veriler için en iyi imputasyon değerlerini öğrenebilir. Bazı algoritmaların ise, kayıp gözlemleri görmezden gelme seçeneği vardır (örn. LightGBM — `use_missing=false`). Ancak, diğer algoritmalar panikleyecek ve kayıp değerlerden şikayet ederek bir hata verecektir (örn. Scikit Learn'deki `LinearRegression` sınıfı). Bu durumda, kayıp gözlemleri işlemeniz ve algoritmaya beslemeden önce temizlemeniz gerekecektir.

5. **Isnull özniteliği**: `isnull` isminde yeni bir öznitelik (değişken/sütun) ekleyerek, herhangi bir diğer değişken için hangi satırların kayıp değerlere sahip olduğunu gösterebilirsiniz. Bu yöntemi kolaylıkla Scikit-Learn kütüphanesindeki `SimpleImputer` fonksiyonundaki `add_indicator` argümanını `True` olarak ayarlayarak gerçekleştirebilirsiniz. Bunu yaparak, ağaç tabanlı yöntemler artık bir değişkende eksik gözlemlerin olduğunu kolaylıkla anlayabilir. Bu yöntemin dezavantajı ise, öznitelik matrisinin (feature matrix) boyutunu arttırmanızdır, bu da, boyut lanetine (curse of dimensionality) neden olabilir.

6. **Ekstrapolasyon (Extrapolation) ve enterpolasyon (interpolation)**: Bilinen veri noktalarının ayrık bir kümesinin aralığındaki diğer gözlemlerden elde edilen değerleri tahmin etmeye çalışırlar.

7. **Doğrusal Regresyon**: Başlamak için, bir korelasyon matrisi kullanılarak kayıp değerlere sahip değişkenin birkaç tahmincisi (predictor) tanımlanır. Yani kayıp değerlere sahip bir değişkene karşı veri setindeki diğer tüm bağımsız değişkenleri kullanırız. Kayıp gözlemin bulunduğu değişken ile yüksek ilişkiye sahip değişkenler, tahminci olarak alınır. Bu tahminciler bağımsız değişkenler ve kayıp gözlemin bulunduğu değişken bağımlı değişken olarak kabul edilir ve bir regresyon denklemi kurulur. Elde edilen bu matematiksel denklem daha sonra kayıp gözlemleri tahmin etmek için kullanılır. Kategorik bir değişkeni impüte etmeye çalıştığımızda da benzer fikir uygulanabilir. Burada, kayıp verilerin yerini alacak değerleri tahmin etmek için bir tahmin modeli (predictive model) oluştururuz. Bu durumda tam veri setini iki kümeye parçalarız. Kayıp değerleri olmayan bir küme eğitim kümesi ve kayıp değerleri olan bir küme test kümesi olarak kullanılır. Daha sonra tahminler için lojistik regresyon ve/veya ANOVA gibi sınıflandırma yöntemlerini kullanabiliriz.

8. **K-En Yakın Komşu**: K en yakın komşu algoritmasını kullanarak, ilk olarak kayıp verili gözleme en yakın K komşuyu buluruz. Daha sonra, mod (kategorik değişken için) ve/veya ortalama (sürekli değişken için) kullanarak, komşulardaki kayıp olmayan gözlemlere dayanarak, kayıp gözlemleri impüte ederiz. Bunun için birkaç olası yaklaşım vardır. En benzer komşuyu bulduğunuz 1NN şemasını kullanabilir ve ardından bu komşunun değerini, kayıp verinin değeri olarak kullanabilirsiniz. Alternatif olarak, kNN'yi k komşu ile kullanabilir ve komşuların ortalamasını veya komşulara olan mesafelerin ağırlık olarak kullanıldığı ağırlıklı ortalamayı alabilirsiniz, böylelikle, yakın komşu, ortalamayı alırken daha fazla ağırlığa sahip olacaktır. Ağırlıklı ortalama yaygın olarak kullanılmaktadır. Hem kesikli öznitelikler hem de sürekli öznitelikler kullanılabilir. Sürekli veriler için Öklid Uzaklığı (Euclidean Distance), Manhattan Uzaklığı (Manhattan Distance) ve Kosinüs Benzerliği (Cosine Similarity) metriklerini kullanabiliriz. Kategorik veriler için genellikle Hamming Uzaklığı (Hamming distance) kullanılır. Karışık (mixed) veri türleri için, mesafeyi hesaplamak için Gower Uzaklığı (Gower Distance) seçilebilir. Gower Uzaklığı, sürekli veri noktları arasındaki mesafeyi hesaplamak için Manhattan Uzaklığı'nıı ve kategorik veri noktaları arasındaki mesafeyi hesaplamak için Dice Uzaklığını (Dice Distance) kullanır. KNN algoritmasının bariz dezavantajlarından biri, tüm küme boyunca benzer örnekleri aradığı için daha büyük veri kümelerini analiz ederken zaman alıcı hale gelmesidir. Tüm eğitim veri setini bellekte sakladığı için hesaplama açısından pahalıdır. Aykırı değerlere karşı da hassastır. Rastgele Ormanlar (Random Forest) algoritması da burada kullanılabilir. Doğrusal olmayan (non-linear) ve kategorik verilerle iyi çalıştığı için sağlam (robust) sonuçlar üretir. Stokastik Regresyon İmputasyonu (Stochastic Regression imputation), Hot-deck imputation, Soft-deck imputation, Multivariate Imputation by Chained Equation (MICE), kullanılacak diğer yaklaşımlardır.

9. **Benzersiz (unique) bir kategori atama**: Bu yaklaşım yalnızca kategorik bir öznitelik için geçerlidir. Kayıp değer için başka bir sınıf atarız, örneğin `missing` kategorisi. Bu strateji, veri kümesine varyansın değişmesine neden olabilecek daha fazla bilgi ekleyecektir. Algoritmanın anlaması için, kategorileri sayısal bir forma dönüştürmek üzere bire-bir kodlama (one-hot encoding) kullanmamız gerekmektedir. Bu da, öznitelik matrisinin (feature matrix) boyutunun artmasına neden olabilir.

Ayrıca, veri sızıntısını önlemek için önce verilerinizi eğitim ve test kümeleri olarak parçalamanız ve ardından impütasyon tekniğini uygulamanız gerektiğini unutmayın.
